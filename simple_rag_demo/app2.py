"""
Streamlit RAG Demo
==================
åŸºäºå·²æœ‰çš„ PDFâ†’Milvus å‘é‡åŒ–æµç¨‹ï¼Œæä¾›ä¸€ä¸ª Web UIï¼Œ
æ”¯æŒï¼š
- åœ¨ Milvus ä¸­æ£€ç´¢æ–‡æ¡£ç‰‡æ®µ
- ç»“åˆ DeepSeek LLM è¿›è¡Œé—®ç­”
- å±•ç¤ºç­”æ¡ˆåŠå¼•ç”¨æ¥æº
è¿è¡Œï¼š
$ streamlit run rag_webui.py
ç¯å¢ƒå˜é‡ï¼š
DEEPSEEK_API_KEY   # DeepSeek å¯†é’¥
DEEPSEEK_API_BASE  # éå¿…å¡«ï¼Œé»˜è®¤ https://api.deepseek.com/v1
"""

import os
import logging
import streamlit as st
from langchain_community.vectorstores import Milvus
from langchain_huggingface import HuggingFaceEmbeddings  # å»ºè®®å®‰è£… langchain-huggingface
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# ---------- é…ç½® ---------- #
DEFAULT_COLLECTION_NAME = "rag_demo_collection"
DEFAULT_MILVUS_URI = "http://localhost:19530"
DEFAULT_MILVUS_TOKEN = "root:Milvus"

EMBEDDING_MODEL_NAME = "BAAI/bge-small-zh-v1.5"

PROMPT_TEMPLATE = (
    """å·²çŸ¥ä¿¡æ¯:\n{context}\n\n"
    "é—®é¢˜: {question}\n\n"
    "è¯·åŸºäºä»¥ä¸Šå·²çŸ¥ä¿¡æ¯ï¼Œç”¨ä¸­æ–‡ç®€æ´å›ç­”é—®é¢˜ï¼›"
    "å¦‚æ— æ³•ä»ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·å›ç­”\"æ— æ³•ä»æ‰€ç»™æ–‡æ¡£ä¸­æ‰¾åˆ°ç­”æ¡ˆ\"ã€‚"""
)

# ---------- æ—¥å¿— ---------- #
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("RAGWebUI")

# ---------- å·¥å…·å‡½æ•° ---------- #
@st.cache_resource(show_spinner="â™»ï¸ æ­£åœ¨åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨ â€¦")
def get_vectorstore(collection_name: str, uri: str, token: str):
    """è¿æ¥ Milvus å¹¶è¿”å› VectorStore å¯¹è±¡"""
    embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={"device": "mps"})
    vs = Milvus(
        embedding_function=embedding,
        collection_name=collection_name,
        connection_args={"uri": uri, "token": token},
        consistency_level="Strong",

    )
    return vs

@st.cache_resource(show_spinner="ğŸ¤– æ­£åœ¨åŠ è½½ LLM â€¦")
def get_llm():
    """åˆå§‹åŒ– DeepSeek ChatOpenAI"""
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        st.error("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEYï¼")
        st.stop()
    llm = ChatOpenAI(
        model_name="deepseek-chat",
        temperature=0.2,
        streaming=True,
        openai_api_key=api_key,
        openai_api_base=os.getenv("DEEPSEEK_API_BASE", "https://api.deepseek.com/v1"),
    )
    return llm

# ---------- Streamlit UI ---------- #
st.set_page_config(page_title="ğŸ“š RAG Demo", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“š RAG é—®ç­”æ¼”ç¤º")

with st.sidebar:
    st.header("ğŸ”§ Milvus é…ç½®")
    collection_name = st.text_input("Collection åç§°", value=DEFAULT_COLLECTION_NAME)
    milvus_uri = st.text_input("Milvus URI", value=DEFAULT_MILVUS_URI)
    milvus_token = st.text_input("Milvus Token", value=DEFAULT_MILVUS_TOKEN, type="password")
    st.divider()
    st.header("ğŸ› ï¸ DeepSeek LLM")
    if "DEEPSEEK_API_KEY" not in os.environ:
        st.text_input("DeepSeek API Key", key="ds_api_key", type="password", on_change=lambda: os.environ.update({"DEEPSEEK_API_KEY": st.session_state.ds_api_key}))
    st.caption("âœ´ï¸ å¦‚æœæœªå¡«å†™ï¼Œåˆ™ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")

# åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨å’Œ LLMï¼ˆå¸¦ç¼“å­˜ï¼‰
vectorstore = get_vectorstore(collection_name, milvus_uri, milvus_token)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})
llm = get_llm()
prompt = PromptTemplate(input_variables=["context", "question"], template=PROMPT_TEMPLATE)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt},
)

# èŠå¤©è¾“å…¥
if "messages" not in st.session_state:
    st.session_state.messages = []  # å­˜å‚¨ (role, content, sources)

for msg in st.session_state.messages:
    with st.chat_message("user") if msg[0] == "user" else st.chat_message("assistant"):
        st.markdown(msg[1])
        if msg[2]:
            with st.expander("å¼•ç”¨æ–‡æ¡£"):
                for i, doc in enumerate(msg[2]):
                    st.markdown(f"**ç‰‡æ®µ {i+1} (page {doc.metadata.get('page_num', '-')})**:\n\n{doc.page_content}")

query = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜ï¼Œå›è½¦å‘é€ â€¦")
if query:
    # è®°å½•ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append(("user", query, None))
    with st.chat_message("user"):
        st.markdown(query)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        try:
            result = qa_chain({"query": query})
            answer = result["result"]
            sources = result.get("source_documents", [])
            placeholder.markdown(answer)
        except Exception as e:
            logger.exception("é—®ç­”å¼‚å¸¸")
            placeholder.error(f"ğŸ’¥ å‡ºé”™: {e}")
            answer, sources = f"å‡ºé”™: {e}", []

    # ä¿å­˜åˆ°å†å²
    st.session_state.messages.append(("assistant", answer, sources))
