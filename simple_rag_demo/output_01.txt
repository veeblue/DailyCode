--------------- 文档加载并清洗 -----------------
原始页面数：8
--------------- 语义切割 -----------------
分割后文档数量: 39

--- 分割段 0 ---
Aug．２０１９
到稿日期:２０１８Ｇ０７Ｇ０６　返修日期:２０１８Ｇ０９Ｇ１５　　本文受国家自然科学基金(６１６７２５２２,６１３７９１０１),国家重点基础研究发展计划(９７３)
(２０１３CB３２９５０２)资助.
杜　威(１９９４－),男,硕士生,主要研究方向为深度强化学习,EＧmail:１３９４４７１１６５＠qq．com;丁世飞(１９６３－),男,博士后,教授,CCF会员,主要
研究方向为机器学习与人工智能,EＧmail:dingsf＠cumt．edu．cn(通信作者).多智能体强化学习综述
杜　威１　丁世飞１,２
(中国科学院计算技术研究所智能信息处理重点实验室　北京１００１９０)２
摘　要　多智能体系统是一种分布式计算技术,可用于解决各种领域的问题,包括机器人系统、分布式决策、交通控制
和商业管理等.多智能体强化学习是多智能体系统研究领域中的一个重要分支,它将强化学习技术、博弈论等应用到
多智能体系统,使得多个智能体能在更高维且动态的真实场景中通过交互和决策完成更错综复杂的任务.文中综述
了多智能体强化学习的最新研究进展与发展动态,首先介绍了多智能体强化学习的基础理论背景,回顾了文献中提出
的多智能体强化学习的学习目标和经典算法,其被分别应用于完全合作、完全竞争和更一般(不合作也不竞争)的任
务.其次,综述了多智能体强化学习的最新进展,近年来随着深度学习技术的成熟,在越来越多的复杂现实场景任务
中,研究人员利用深度学习技术来自动学习海量输入数据的抽象特征,并以此来优化强化学习问题中智能体的决策.
近期,研究人员结合深度学习等技术,从可扩展性、智能体意图、奖励机制、环境框架等不同方面对算法进行了改进和
创新.最后,对多智能体强化学习的应用前景和发展趋势进行了总结与展望.目前多智能体强化学习在机器人系统、

--- 分割段 1 ---
创新.最后,对多智能体强化学习的应用前景和发展趋势进行了总结与展望.目前多智能体强化学习在机器人系统、
人机博弈、自动驾驶等领域取得了不错的进展,未来将被更广泛地应用于资源管理、交通系统、医疗、金融等各个领域.
关键词　强化学习,多智能体系统,博弈论,多智能体强化学习,深度学习
中图法分类号　TP１８１　　　文献标识码　A　　　DOI　１０．１１８９６/j．issn．１００２Ｇ１３７X．２０１９．０８．００１
OverviewonMultiＧagentReinforcementLearning
DUWei１　DINGShiＧfei１,２
(SchoolofComputerScienceandTechnology,ChinaUniversityofMiningandTechnology,Xuzhou,Jiangsu２２１１１６,China)１
(KeyLaboratoryofIntelligentInformationProcessing,InstituteofComputingTechnology,ChineseAcademyofSciences,Beijing１００１９０,China)２
Abstract　MultiＧagentsystemisadistributedcomputingtechnology,whichcanbeusedtosolveproblemsinvarious
fields,includingrobotsystem,distributeddecisionＧmaking,trafficcontrolandbusinessmanagement．MultiＧagentreinＧ

--- 分割段 2 ---
forcementlearningisanimportantbranchinthefieldofmultiＧagentsystemresearch．Itappliesreinforcementlearning
technologyandgametheorytomultiＧagentsystems,enablingmultipleagentstocompletemorecomplicatedtasks
throughinteractionanddecisionＧmakinginhigherＧdimensionalanddynamicrealscenes．Thispaperreviewedtherecent
researchprogressanddevelopmentofmultiＧagentreinforcementlearning．Firstly,thetheoreticalbackgroundofmultiＧ
agentreinforcementlearningwasintroduced,andthelearningobjectivesandclassicalalgorithmsofmultiＧagentreinＧ
forcementlearningproposedintheliteraturewerereviewed,whicharerespectivelyappliedtocompletecooperation,
completecompetitionandmoregeneral(neithercooperationnorcompetition)tasks．Secondly,thelatestdevelopmentof

--- 分割段 3 ---
multiＧagentreinforcementlearningwassummarized．Withthematurityofdeeplearningtechnologyinrecentyears,in
moreandmorecomplexrealisticscenetasks,researchersusedeeplearningtechnologytoautomaticallylearnabstract
featuresofmassiveinputdata,andthenusethesedatatooptimizethedecisionＧmakingofagentsinreinforcementlearＧ
ning．Recently,researchershavecombineddeeplearningandothertechnologiestoimproveandinnovatealgorithmsin
differentaspects,suchasscalability,agentintent,incentivemechanism,andenvironmentalframework．Attheendofthis
paper,theprospectoftheapplicationofmultiＧagentreinforcementlearningweresummarized．MultiＧagentreinforcement
learninghasmadegoodprogressinthefieldsofrobotsystem,manＧmachinegameandautonomousdriving,andwillbe

--- 分割段 4 ---
learninghasmadegoodprogressinthefieldsofrobotsystem,manＧmachinegameandautonomousdriving,andwillbe
appliedinthefieldsofresourcemanagement,transportationsystem,medicaltreatmentandfinanceinthefuture．
Keywords　Reinforcementlearning,MultiＧagentsystems,Gametheory,MultiＧagentreinforcementlearning,Deeplearning

--- 分割段 5 ---
多智能体系统(MultiＧagentSystem,MAS)是多个智能体
组成的集合,其目标是将大而复杂的系统建设成小而彼此互
相通信协调的易于管理的系统.多智能体系统自２０世纪７０
年代被提出以来,就在智能机器人、交通控制、分布式决策、商
业管理、软件开发、虚拟现实等各个领域迅速地得到了应用,
目前已经成为一种对复杂系统进行分析与模拟的工具.多智
能体系统由分布式人工智能演化而来,其研究目的是解决大
规模的、复杂的现实问题.在现实问题中,单智能体的决策能
力远远不够.使用一个中心化的智能体解决问题时,会遇到
各种资源和条件的限制,导致单个智能体无法应对错综复杂
的现实环境;而使用多个智能体相互协作可以解决很多问
题[１].强化学习(ReinforcementLearning,RL)是机器学习的
一种重要方法,它是一种以环境反馈作为输入目标,用试错方
法发现最优行为策略的学习方法.在强化学习的数学基础研
究取得了突破性的进展后,对强化学习的研究和应用日益增
多[２].目前,强化学习已被广泛应用于手工业制造、机器人控
制、优化与调度、仿真模拟、游戏博弈等领域[３].目前,结合多
智能体系统和强化学习方法形成的多智能体强化学习正逐渐
成为强化学习领域的研究热点之一,并在各个领域得到广泛
应用[４Ｇ６].
多智能体强化学习(MultiＧAgentReinforcementLearＧ
ning,MARL)是将强化学习的思想和算法应用到多智能体系
统中.２０世纪９０年代,Littman[７]提出了以马尔可夫决策过
程(MarkovDecisionProcess,MDP)为环境框架的MARL,为
解决大部分强化学习问题提供了一个简单明确的数学框架,
后来研究者们大多在这个模型的基础上进行了更进一步的研
究.最近,随着深度学习的成功,人们将深度学习的方法与传

--- 分割段 6 ---
解决大部分强化学习问题提供了一个简单明确的数学框架,
后来研究者们大多在这个模型的基础上进行了更进一步的研
究.最近,随着深度学习的成功,人们将深度学习的方法与传
统的强化学习算法相结合,形成了许多深度强化学习算法,使
单智能体强化学习的研究和应用得到迅速发展.比如,DeepＧ
Mind公司研制出的围棋博弈系统AlphaGO已经在围棋领域
战胜了人类顶级选手,并以较大优势取得了胜利,这极大地震
撼了社会各界[８],也促使研究人员在多智能体强化学习领域
投入更多的精力.以DeepMind,OpenAI公司为代表的企业
和众多高校纷纷开发MARL的新算法,并将其应用到实际生
活中,目前主要应用于机器人系统[９Ｇ１０]、人机对弈[１１Ｇ１３]、自动
驾驶[１４]、互联网广告[１５]和资源利用[１６Ｇ１７]等领域.
本文第２节简单介绍了多智能体强化学习的基础理论;
第３节结合深度强化学习的最新算法,从可扩展性、智能体意
图等不同角度对多智能体强化学习的最新研究进展进行了综
述;第４节对多智能体强化学习在现实领域中的应用和前景
进行了探讨;最后总结全文.
２　MARL的基础理论
２．１　单智能体强化学习
根据反馈的不同,机器学习方法可以分为监督学习、非监
督学习和强化学习.强化学习的智能体通过不断地与动态环
境交互和不断地试错来进行学习.如图１所示,在每一个过
程中,智能体感知环境的完整状态并采取行动,然后使环境进入一个新的状态.之后智能体会收到一个反馈,用于评估这
次状态转移.这种反馈相比监督学习中的样本标记,其信息
量要少且具有延时性.这是因为在监督学习中,智能体总是
被告知采用什么动作是正确的;同时,这种反馈的信息量又比
没有标记的无监督学习要多,因为在无监督学习中,智能体需
要自己去发现正确的行动,并且得不到关于这次行动的任何
明确的反馈[１８Ｇ１９].

--- 分割段 7 ---
被告知采用什么动作是正确的;同时,这种反馈的信息量又比
没有标记的无监督学习要多,因为在无监督学习中,智能体需
要自己去发现正确的行动,并且得不到关于这次行动的任何
明确的反馈[１８Ｇ１９].
单智能体强化学习的目标是智能体通过与环境的不断交
互来学习一个最优的策略,使累计回馈最大.图１中,一个完
整的强化学习任务有几个重要的组成部分:动作、状态、反馈、
环境.强化学习的环境是马尔可夫过程,执行策略及价值函
数是决策过程中比较重要的概念,最终强化学习的目标可以
转化为求解最优贝尔曼方程.
图１　强化学习的基本框架
Fig．１　Basicframeworkofreinforcementlearning
２．１．１　马尔可夫决策过程
强化学习的环境是用马尔可夫决策过程描述的.马尔可
夫决策过程是一种无记忆的随机过程,它对完全可观测环境
进行描述.
马尔可夫决策过程是这样一个元组:‹S,A,R,P›,其中,
S表示状态空间,A表示动作空间,R表示奖励值,P表示转
移概率,R与P都与具体行为A对应,数学表达式如下:
SS′＝P[St＋１＝S′|St＝s,At＝a] (１)
S＝E[Rt＋１|St＝s,At＝a] (２)
其中,Pa
SS′是状态转移概率函数,Ra
S是期望奖励函数.智能
体转移到下一个状态的概率以及得到的奖励与当前状态和在
此状态下采取的行为相关.
２．１．２　策略
智能体采取的策略是概率的集合或分布,其元素π(a|s)＝
P[At＝a|St＝s]为对过程中某一状态s采取可能行为a的概
率.π仅与当前的状态有关,与历史信息无关;同时,某一确
定的策略是静态的,与时间无关.给定一个MDP[S,A,R,
P]和一个策略π,智能体在MDP环境下执行策略π时的概率
函数和奖励函数为:
SS′＝∑
a∈Aπ(a|s)Pa
SS′ (３)
a∈Aπ(a|s)Ra
S (４)

--- 分割段 8 ---
P]和一个策略π,智能体在MDP环境下执行策略π时的概率
函数和奖励函数为:
SS′＝∑
a∈Aπ(a|s)Pa
SS′ (３)
a∈Aπ(a|s)Ra
S (４)
其中,Pa
SS′是指执行策略π时,执行某一行为的概率与采取这
一行为的状态转移概率的乘积和,代表了执行策略π时智能
体从s转移到s′的概率;Rπ
S指执行策略π时得到的奖励是所
有可能执行行为的概率与采取这一行为得到的奖励的乘
２．１．３　最优价值函数
MDP下的基于策略π的状态价值函数vπ(s)表示执行策

--- 分割段 9 ---
vπ(s)＝Eπ[Gt|St＝s] (５)
最优状态价值函数定义为:
v∗＝maxvπ(s) (６)
MDP下的行为价值函数qπ(s,a)被用于衡量在当前策略
π下对当前状态s执行行为a的最优价值.其数学表达式为:
qπ(s,a)＝Eπ[Gt|St＝s,At＝a] (７)
最优行为价值函数定义为:
q∗(s,a)＝maxqπ(s,a) (８)
２．１．４　最优贝尔曼方程
智能体通过最大化最优价值函数,将得到以下两个方程,
其也被称为贝尔曼方程.
vπ(s)＝Eπ[Rt＋１＋γVπ(St＋１)|St＝s] (９)
Qπ(s,a)＝Eπ[Rt＋１＋γQπ(St＋１,At＋１)|St＝s,At＝a]
其中,γ是折扣因子,用于保证越后面的回报对回报函数的影
响越小,刻画了未来回报的不确定性,同时也使得回报函数是
有界的.通过式(９)、式(１０)可以看出,贝尔曼方程由两部分
组成:１)该状态的即时奖励期望;２)下一时刻状态的价值期望
乘上衰减系数.贝尔曼方程是对于某一个给定策略,求其状态
价值函数和行为价值函数,也即对某一策略进行评估,而强化
学习最终的目标是寻找最优策略,于是引入最优贝尔曼方程:
Vπ(s)＝maxRa
s′∈SPa
SS′v∗(s′) (１１)
Q∗(s,a)＝Ra
s′∈SPa
SS′maxq∗(s′,a′) (１２)
强化学习的问题最终可以转化为求解最优贝尔曼方程.
由于方程是非线性的,因此需要通过一些方法来求解.由此
产生了两个重要分类,所有强化学习问题的解决方法基本都
可以归结为基于价值的和基于策略的,其中基于价值函数的
代表方法是QＧlearning[２０].QＧlearning最早由Watkins和
Dayan于１９９２年提出,它的价值函数的迭代方式为:
Q(s,a)←(１－α)Q(s,a)＋α[r＋γmaxQ(s′,a′)](１３)

--- 分割段 10 ---
Dayan于１９９２年提出,它的价值函数的迭代方式为:
Q(s,a)←(１－α)Q(s,a)＋α[r＋γmaxQ(s′,a′)](１３)
其中,α是学习速率.
２．２　多智能体强化学习
多智能体系统由分布式人工智能演变而来,具有自主性、
分布性、协调性等特点,并具备学习能力、推理能力和自组织
能力.尽管智能体的概念在２０世纪４０年代就已经出现,但
在２０世纪７０年代之前将多个智能体作为一个整体系统的研
究却很少.直到２０世纪８０年代后期,分布式人工智能开始
显著发展,建立在博弈论概念之上的分布式人工智能逐渐演
变并最终形成了多智能体系统.之后,在多智能体系统的分
布式问题求解模型中,分布式约束推理(DCR)模型(如分布
式约束满足问题(DCSP)和分布式约束优化问题(DCOP)的
研究和使用较为广泛.DCR在各种分布式问题上都有应用,
比如分布式传感器任务分配和分布式会议安排策略等.
最近,大量的研究关注于寻找解决多智能体系统不确定
性问题的方法.在各种模型和求解方法中,分布式马尔可夫
决策过程(DecＧMDP)和分布式部分可观测马尔可夫决策过
程(DecＧPOMDP)是不确定性情形下最常用的两种模型.不
幸的是,求解DecＧPOMDP通常是很难的.强化学习的发展
给多智能体系统解决不确定性等问题提供了一种全新的思
路,多智能体强化学习正逐渐成为MAS众多子领域中最受关注的领域.下面对多智能体强化学习的基本概念和经典算
法进行了简单的介绍.
首先,MARL的环境是以马尔可夫决策过程为基础的随
机博弈框架,它是这样一个元组‹S,A１,􀆺,An,R１,􀆺,Rn,
P›.其中,n指多智能体的数量;A是所有智能体的联合动作
空间集,A＝A１×􀆺×An;Ri是每个智能体的奖励函数,Ri:
S×A×S→R;P是状态转移函数,P:S×A×S→[０,１].我
们假设奖励函数是有界的.

--- 分割段 11 ---
空间集,A＝A１×􀆺×An;Ri是每个智能体的奖励函数,Ri:
S×A×S→R;P是状态转移函数,P:S×A×S→[０,１].我
们假设奖励函数是有界的.
在多智能体情况下,状态转换是所有智能体共同行动的
结果,因此智能体的奖励也取决于联合策略.定义策略H是
智能体的联合策略Hi:S×A→H,相应地,每个智能体的奖
i＝E[Rt＋１|St＝s,At,i＝a,H] (１４)
其贝尔曼方程为:
i(s)＝EH
i[Rt＋１＋γVH
i(St＋１)|St＝s] (１５)
i(s,a)＝EH
i[Rt＋１＋γQH
i(St＋１,At＋１)|St＝s,At＝a]
根据任务类型,多智能体强化学习可以分为完全合作、完
全竞争和混合型.在完全合作的随机博弈中,奖励函数对于
所有智能体都是相同的,即R１＝R２＝􀆺＝Rn,因此回报也相
同,多智能体的目标就是最大化共同回报.如果n＝２,R１＝
－R２,那么两个智能体有相反的目标,随机博弈就是完全竞
争的.此外,存在既不完全竞争也不完全合作的策略,称其为
混合策略.
在完全合作的随机博弈中,回报可以共同最大化.在其
他情况下,智能体的回报通常是不同且相关的,它们不可能独
立最大化.因此,指定良好的、通用的MARL目标是一个难
题.回顾已有文献中对学习目标的定义,其主要可以概括为
两个方面:稳定性和适应性.
稳定性指智能体的学习动力的稳定性以及策略会收敛至
固定.适应性确保智能体的表现不会因为其他智能体改变策
略而下降.收敛至均衡态是稳定性的基本要求,即所有智能
体的策略收敛至协调平衡状态,最常用的是纳什均衡.适应
性体现在理性或无悔两个准则上.理性是指当其他智能体稳
定时,智能体会收敛于最优反馈;无悔是指最终收敛的策略的
回报不能差于任何其他策略的回报.在确定学习目标以后,
我们根据不同的任务类型对经典的强化学习算法做了分类和

--- 分割段 12 ---
定时,智能体会收敛于最优反馈;无悔是指最终收敛的策略的
回报不能差于任何其他策略的回报.在确定学习目标以后,
我们根据不同的任务类型对经典的强化学习算法做了分类和
２．２．１　完全合作
在完全合作的随机博弈中,智能体有相同的奖励函数,此
时学习目标可以表述为:
Qt＋１(st,at)＝Qt(st,at)＋α[rt＋１＋γmaxQt＋１(st＋１,a′)－
Qt(st,at)] (１７)
与单智能体一样,智能体会采用贪心策略来最大化回报:
hi(x)＝argmax
ai　max
ai􀆺an　Q∗(s,a) (１８)
然而,各智能体在做决策时是非独立的,即使它们平行地
学习一个共同的目标,因此考虑智能体之间的协作问题变得
很有必要.TeamＧQ算法[２１]通过假设最优的联合行动是唯一
的来避免协作问题.DistributedＧQ算法[２２]在不假设协调的３ 第８期 杜　威,等:多智能体强化学习综述

--- 分割段 13 ---
情况下以有限的计算量解决协作任务,其计算复杂度与单智
能体Q学习的计算复杂度相似.然而,该算法只适用于具有
非负报酬函数的确定性问题.上述算法存在一些局限,即它
们都依赖于对状态的精确测量,一些还需要精确测量其他智
能体的作用,并且还会受维数灾难的影响.
２．２．２　完全竞争
在完全竞争的随机博弈中(对于两个智能体,即R１＝R２,
可以应用极小极大化原则):在最坏情况下假设最大化一个智
能体的回报,这个假设是对手将始终努力使其回报最小化.
minimaxＧQ算法采用极小极大原理来计算阶段游戏的策略和
值,以及类似于Q学习的时序差分规则.下面给出智能体１
h１,t(st,􀅰)＝argm１(Qt,xt) (１９)
Qt＋１(st,a１,t,a２,t)＝Qt(st,a１,t,a２,t)＋α[rk＋１＋γm１(Qt,
at＋１)－Qt(st,a１,t,a２,t)] (２０)
其中,m１是智能体１的极小极大化回报.
m１(Q,s)＝max
h１(s,􀅰)　min
a１h１(x,a１)Q(s,a１,a２) (２１)其中,在状态s时的智能体１的随机策略由h１(s,􀅰)表示,点代
表动作参数.最优化问题可以由线性规划解决.
２．２．３　混合型任务
在混合随机博弈中,智能体的奖励函数不受约束,这种模
式最适合自私的智能体.博弈论均衡概念在混合随机博弈中
运用得最多,该类别中的大量算法仅针对静态任务.像Q学
习这样的单智能体算法可以直接应用到混合型任务中.参数
的更新需要使用所有智能体的Q表,因此每个智能体都要复
制其他智能体的Q表,这要求所有的智能体使用相同的算法
并且可以测量所有的动作和奖励.即使有了这些假设,当不
同智能体求得的策略不唯一时,也会出现均衡选择问题.一
种常用的方法是NashQＧlearning,此外还有相关平衡Q学习

--- 分割段 14 ---
并且可以测量所有的动作和奖励.即使有了这些假设,当不
同智能体求得的策略不唯一时,也会出现均衡选择问题.一
种常用的方法是NashQＧlearning,此外还有相关平衡Q学习
(CEＧQ)[２３]或不对称Q学习(AsymmetricQＧlearning)[２４],它们
可以分别通过使用相关或Stackelberg(前导Ｇ跟随)平衡来解决
均衡问题.对于不对称Q学习,跟随者不需要对领导者的Q
表进行建模,但是领导者必须知道追随者如何选择其行动.
表１简单地对各种算法进行了对比和总结.
表１　基于博弈的经典MARL算法的对比
Table１　ComparisonofclassicMARLalgorithmbasedongametheory
算法 值函数更新 动作选择
QＧlearning Q(s,a)←(１－α)Q(s,a)＋α[r＋γmaxQ(s′,a′)] εＧ贪心策略,或基于玻尔兹曼机分布的动作选择
MinimaxQQ(s,a,o)←(１－α)Q(s,a,o)＋α[r＋γV(s′)]
V(s′)＝max
π　min
o　∑aQ(s′,a,o)π(s′,a′)基于当前学习到的策略π选择动作
NashQ Q(s,a)←(１－α)Q(s,a)＋α[ri＋γNashQi(s′)] 基于当前博弈的纳什均衡选择动作
CEＧQ Q(s,a)←(１－α)Q(s,a)＋α[ri＋γCEQi(s′)] 基于当前博弈的相关均衡选择动作
注:o代表相互博弈的智能体的对手采取的动作
然而,传统的MARL算法多适用于小规模的问题,很少
有算法能应用于信息不完整或不确定的环境.提高MARL
对实际问题的适应性是一个必不可少的研究步骤[２５].
３　MARL的研究进展
经典的MARL算法通常仅适用于小问题,如静态游戏和
小网格世界,而在现实的多智能体问题中,状态和动作空间很

--- 分割段 15 ---
对实际问题的适应性是一个必不可少的研究步骤[２５].
３　MARL的研究进展
经典的MARL算法通常仅适用于小问题,如静态游戏和
小网格世界,而在现实的多智能体问题中,状态和动作空间很
大甚至是连续的.很少有传统算法能够适用于不完整的、不
确定的环境.可扩展性和处理不完全信息环境也是之前单智
能体强化学习中未解决的问题.然而,随着深度学习的发展,
单智能体的可扩展性等问题在一定程度上得到了解决.借助
深度学习快速发展的红利,研究人员结合深度学习等技术,从
可扩展性、智能体意图、奖励机制、环境框架等诸多方面对多
智能体算法进行了改进和创新.
３．１　深度强化学习的主要进展
３．１．１　深度Q网络
深度Q网络(DeepQＧNetwork,DQN)是由Mnih等[２６]提
出的.它依托强化学习中经典的Q学习,用一个深度网络近
似价值函数为深度网络提供目标值,不断更新网络直至收敛.
其中涉及３项关键技术:１)用经验重放技术打破了样本间的
关联性,将采集到的样本先放入样本池,然后从池中随机选出
一个样本用于网络训练;２)设置了目标网络来单独处理时间
差分算法中的TD偏差,使训练的稳定性和收敛性得到极大
的提高;３)利用卷积神经网络逼近行为值函数.其他学者也围绕DQN做了许多研究和改进.Hasselt等[２７]提出了双
DQN算法,使得值估计过于乐观这一问题得到解决.Schaul
等[２８]使用了经验优先回放技术,对经验的优先次序进行处
理.此外,Osband等[２９],Munos等[３０],FrancoisＧlavet[３１]等也
分别从其他角度对DQN提出了改进.
３．１．２　演员评论家算法
演员评论家算法(ActorＧCritic,AC)的架构可以追溯到
３０~４０年前(见图２).最早,Witten于１９７７年提出了类似

--- 分割段 16 ---
分别从其他角度对DQN提出了改进.
３．１．２　演员评论家算法
演员评论家算法(ActorＧCritic,AC)的架构可以追溯到
３０~４０年前(见图２).最早,Witten于１９７７年提出了类似
AC算法的方法;然后,Barto,Sutton和Anderson等于１９８３
年引入了AC架构.但是,AC算法的研究难度和一些历史偶
然因素使得之后学术界开始将研究重点转向基于价值的方
法.之后的一段时间里,基于价值的方法和基于策略的方法
都有了蓬勃的发展.AC算法结合了两者的发展红利,在理论
和实践方面再次有了长足的发展.
图２　演员评论家算法的基本框架
Fig．２　BasicframeworkofactorＧcritialgorithm
该结构包含两个网络:一个策略网络(Actor)和一个价值
网络(Critic).策略网络输出动作,价值网络评判动作.策略
网络通过梯度计算公式进行更新,而价值网络根据目标值进

--- 分割段 17 ---
梯度的方法,这使得它能在连续动作或更高维动作空间中选
取合适的动作;而Q学习难以实现这个目标,甚至会瘫痪.
相比单纯策略梯度,AC算法应用了Q学习或其他策略评估
方法,使得AC算法能进行单步更新而不是回合更新,比单纯
的策略梯度的效率更高.
３．１．３　深度确定性策略梯度
深度Q网络是一种基于价值函数的方法,难以应对大的
连续动作空间,无法输出离散状态动作值.Lillicrap等[３２]于
２０１５年提出的深度确定性策略梯度(DeepDeterministicPoliＧ
cyGradients,DDPG)是基于上述AC算法的,结合确定性策
略梯度算法(DeterministicPolicyGradients,DPG),在动作输
出方面采用一个网络来拟合策略函数,直接输出动作,可以应
对连续动作的输出以及更大的动作空间.此外,AC模型还
衍生出很多种算法,如异步优势行动评论家算法(A３C)[３３]和
分布式近似策略优化算法(DPPO)[３４Ｇ３５].A３C算法由Mnih
于２０１６年提出,在DQN中为了破坏训练样本之间的相关
性,采用了经验重放技术,即把训练样本缓存起来,每次训练
时从中随机抽取一个minibatch.在A３C中,利用多线程并
行地去采集数据,每个线程以一个独立的智能体形式去搜索
独立环境;同时,每个智能体还可以平行地利用不同的探索策
略进行采样,这样每个线程得到的样本天然不相关,而且采样
速度也更快.
３．２　MARL算法进展
３．２．１　可扩展性
可扩展性是目前MARL领域的核心关注点.早期的多
智能体强化学习研究都是应用于小问题,在离散的动作和状
态空间上对Q学习算法进行改进和完善.但是将算法扩展
到现实的多智能体问题时,其中状态和动作空间是很大甚至
连续的,Q函数的表格存储变得不切实际或不可能.受强化
学习与深度学习技术相结合的启发,研究人员把DQN和

--- 分割段 18 ---
到现实的多智能体问题时,其中状态和动作空间是很大甚至
连续的,Q函数的表格存储变得不切实际或不可能.受强化
学习与深度学习技术相结合的启发,研究人员把DQN和
DPG等技术应用到多智能体强化学习中.近期的大部分研
究集中于此.
将DQN泛化到MARL存在的最大问题是经验重放的
方法变得不再适用,如果不知道其他智能体的状态,那么不同
情况下自身的状态转移概率会不同.Foerster等[３６]提出了两
种方法对经验重放方法进行改进,使多智能体在使用经验重
放技术时更稳定并更具兼容性.一方面使用一个运用重要性
抽样的多智能体变量来自然衰减过时数据,将经验重放的数
据解释为环境外数据[３７].由于较旧的数据往往会产生较低
的重要性,因此这种方法能避免非固定重放经验数据产生的
混淆.另一方面,让每个智能体通过观察其他智能体的决策
来推测其他智能体的行为,这一方法能适用于更大范围的深
度网络.其算法在传统多智能体算法IQL[３８Ｇ４１]上做了改进,
IQL算法的全称是independentQＧlearning,顾名思义,其是对
每个智能体独立地执行一次QＧlearning算法,将其他智能体
视为环境的一部分.IQL的点在于它忽略了这样一个事实:
这些智能体的策略会随着时间的推移而变化,使其自身的Q
函数变得非平稳.而研究人员通过让每个智能体推断其他智
能体的行为并学习一种策略来规避其他智能体的策略,从而
改善了IQL的非平稳性.Yang等[４２]提出了平均场强化学习对多智能体系统中的
动态行为进行建模,有效地回应了相邻智能体的平均效应,通
过将多智能体问题简化为两个智能体的问题,解决了智能体
数量增加导致的维数和指数增加的问题.他们没有分别考虑
单个智能体对其他个体产生的不同影响,只是将领域内所有
其他个体的影响用一个均值来代替,这样,对于每个个体,只

--- 分割段 19 ---
数量增加导致的维数和指数增加的问题.他们没有分别考虑
单个智能体对其他个体产生的不同影响,只是将领域内所有
其他个体的影响用一个均值来代替,这样,对于每个个体,只
需要考虑个体与这个均值的交互作用即可.应用平均场论
后,学习在两个智能体之间是相互促进的:单个智能体的最优
策略的学习是基于智能体群体动态的;同时,集体的动态也根
据个体的策略进行更新.
还有研究人员对DQN在多智能体上的应用做了很多工
作.Tuyls等[４３]提出了LDQN算法,将宽容政策引入深度Q
网络,采用宽大处理方法更新消极政策,使收敛性和稳定性都
得到提高.Zheng等[４４]提出了一个多智能体深度强化学习
框架,称其为加权双深Q网络(WDDQN).通过利用深度神
经网络和加权双估计器,WDDQN不仅可以有效减小偏差,而
且可以扩展到许多深度强化学习场景.根据经验,WDDQN
在随机合作环境中的性能和收敛效果优于现有的DRL算法
(双DQN)和MARL算法(宽松Q学习).Tampuu等[４５]证明
了由自主深度Q网络控制的智能体能够从原始感官数据中
学习双人视频游戏.
将DPG应用到更高维的多智能体环境中时面临的最大
问题是,环境的不断变化进一步增大了学习的方差.Song
等[４６]提出了一种新的多智能体策略梯度算法,该算法解决了
通常观测到的高方差梯度估计问题,在粒子高复杂环境中可
以有效优化多机器人协作控制任务.Wai等[４７]为了使基于
策略的MARL更具可扩展性和鲁棒性,提出了一种分散的局
部交换方案,其中每个智能体只通过网络与邻居通信.这是
一种双重平均方案,其中每个智能体分别在空间和时间上迭
代,以分别合并相邻梯度信息和本地奖励信息.Abouheaf
等[４８]针对基于图交互的多智能体系统,提出了一种基于策略
迭代的在线自适应强化学习方法.该方法利用降值函数求解

--- 分割段 20 ---
代,以分别合并相邻梯度信息和本地奖励信息.Abouheaf
等[４８]针对基于图交互的多智能体系统,提出了一种基于策略
迭代的在线自适应强化学习方法.该方法利用降值函数求解
多智能体系统的耦合贝尔曼方程,在用策略迭代方法进行更
新时,考虑了降值函数以降低计算复杂度,解决了大规模优化
还有一些学者开始将基于AC框架的算法应用到多智能
体强化学习中.OpenAI提出的MADDPG算法,实质上是
DDPG算法的一种延伸和扩展.MADDPG的基本架构与
DDPG一样,每一个智能体使用自己独立的表演者,通过自己
观察状态输出,确定动作,同时训练数据也只能使用自己产生
的训练数据.每个智能体同时也对应一个评论家,不同的是
这个评论家将同时接受所有表演者的数据,这种中心化的评
论家存在多个.该算法基本解决了DQN经验重放不再适用
和DPG方差过大的问题.这种以AC为架构的算法与多智
能体强化学习的结合是未来研究的一个重要方向.
３．２．２　智能体意图
人类物种的成功归功于人们对物质世界和社会环境的显
著适应性.人类社会智能赋予我们推理其他人心态的能力,
这种心理状态推理广泛影响着我们日常生活中的决策.例
如,安全驾驶要求我们推断其他驾驶员的意图并做出相应的
决定.这种微妙的意图决策(心智理论)行为在人类活动中无５ 第８期 杜　威,等:多智能体强化学习综述

--- 分割段 21 ---
处不在,但即使在最先进的多智能体系统中也很难实现.
于是,人们尝试了很多关于智能体自主意图的研究工作.
Qi等[４９]提出了一种意图感知的多智能体规划框架以及学习
算法,在此框架下,智能体计划在目标空间中最大化预期效
用,并且在规划过程中考虑了其他智能体的意图;他们提出了
一个简单但有效的效用函数的近似函数,而不是将学习问题
公式化为部分可观察的马尔可夫决策过程.Raileanu等[５０]提
出了一种新的学习方法SOM(SelfOtherＧModeling),其中智
能体使用自己的策略来预测另一个智能体的策略,并在线更
新其隐藏的状态.Rabinowitz等[５１]提出了一种心智理论的
神经网络(TheoryofMindneuralnetwork,ToMnet),该网络
使用元学习,通过观察其行为来构建智能体所遇到的模型.
研究者将ToMnet应用到简单的格子环境中的智能体上,研
究结果表明它可以学习和模拟来自不同群体的行为,并且它
通过了经典的ToM任务测试.可以通过训练ToMnet来预
测其他智能体的意图,并且其明确地揭示了其他智能体的错
３．２．３　奖励机制
多智能体系统中的奖励机制比单智能体中的奖励机制更
加复杂.目前,在MARL中应用的一些奖励信号存在问题.
比如,全局奖励信号向所有智能体分配相同的奖励而不区分
它们的贡献,这可能会鼓励懒惰的智能体;而本地奖励信号仅
基于个体行为向每个智能体提供不同的本地奖励,这又可能
会产生自私的智能体.如何设计一个合适的奖励信号来加速
学习和稳定收敛,是一个关键问题.有研究人员设计了一些
混合的奖励信号,智能体通过混合信号能学习到更好的策略,
实验结果也得到提高.除了混合信号外,Omidshafiei１等[５２]
的研究表明在智能体之间添加通信机制能显著提高团队的最
终绩效,他们第一次在多智能体强化学习中引入了教学机制.

--- 分割段 22 ---
实验结果也得到提高.除了混合信号外,Omidshafiei１等[５２]
的研究表明在智能体之间添加通信机制能显著提高团队的最
终绩效,他们第一次在多智能体强化学习中引入了教学机制.
其主要贡献是将元学习的思想引入合作的多智能体强化学习
中,将智能体的学习进度作为奖励反馈机制.
４　MARL的应用和前景
近几年,MARL在许多领域都得到了实际应用,目前在
机器人系统、人机博弈和自动驾驶等领域开展的研究较多.
在机器人系统领域,Gu等[５３]提出了一种基于离线策略的深
度强化学习算法,其可以有效地训练真实的物理机器人.机
器人可以在没有任何演示或手动设计的情况下,学习各种仿
真与复杂的操作技巧.Foerster等[１０]将多智能体强化学习方
法应用到机器人交流领域,他们采用集中学习分散执行的方
式,第一次实现了机器人之间的深层次交流.Duan等[５４]在
机器人控制上的实践工作对多智能体强化学习的应用也具有
启发意义.人机博弈一直都是人工智能领域最具挑战性也最
令人兴奋的工作,２０１８年OpenAI和DeepMind相继取得重
大进展.OpenAI在实时５v５策略游戏dota２中战胜人类顶
级玩家;DeepMind在复杂的第一人称多人游戏QuakeIII中
达到人类水平,还能与人类玩家合作[１１Ｇ１３].在自动驾驶领域,
ShalevＧShwartz等[１４]针对自动驾驶的安全性和环境的不可预
测性问题进行了改进和优化,并展示了如何在没有MDP假
设的情况下使用策略梯度迭代,以及用随机梯度上升来最小
化梯度估计的方差.可以预见的是,未来MARL还将被更广泛地应用到各行
各业,如互联网、资源管理、交通系统、医疗和金融等领域.目
前,在互联网领域,Ji等[１５]将聚类的思想与MARL方法相结
合,面对大量的广告商,对即时在线投标的性能进行了优化;

--- 分割段 23 ---
各业,如互联网、资源管理、交通系统、医疗和金融等领域.目
前,在互联网领域,Ji等[１５]将聚类的思想与MARL方法相结
合,面对大量的广告商,对即时在线投标的性能进行了优化;
为了平衡广告商之间的竞争与合作,提出并实现了一种实用
的分布式协调多智能体竞标算法.在资源管理方面,Xi等[１６]
提出了一种MARL新算法,新算法不基于马尔可夫假设,具
有更快的收敛速度和更强的鲁棒性,使得电网系统能够在更
复杂的条件下提高新能源的利用率.Perolat等[１７]运用部分
马尔可夫观测模型对公共资源的占用主体进行了建模,揭示
了排他性、可持续性和不平等性之间的关系,并提出了解决方
案,提高了资源管理能力.Kofinas等[５５]提出的模糊Q学习
方法有效地提高了分散式微电网的能量管理能力.在交通控
制领域,Chen等[５６]提出了一种基于MARL的协同控制框
架,并用其来实时缓解公交车道上的公交拥挤.Vidhate
等[５７]提出了一种基于协同多智能体强化学习的交通流模型
用于控制优化交通系统,模型能够很好地处理未知的复杂状
态.目前,多智能体强化学习应用在医疗和金融等领域的研
究还较少,需要人们进行更进一步的探索.
MARL虽然已经在诸多领域中实现了应用,但依然存在
很多问题.在MARL未来的研究工作和实际应用中,有一些
方向需要进行进一步的关注和探索.首先,可扩展性依然是
最核心的问题.目前,MARL在电脑游戏、人机博弈(包括小
型机器人体系)中已经取得了不错的成果,其中DeepMind公
司和OpenAI公司做出了重要的贡献.但是,MARL在自动
驾驶、船舶制造、商业交易、资源配置等大型机器人体系中的
应用还不够成熟,因此将MARL应用到海量智能体以及更复
杂的环境中是未来研究的重要方向.其次,现在普遍的多智
能体强化学习都是基于马尔可夫随机过程环境的,它给许多

--- 分割段 24 ---
应用还不够成熟,因此将MARL应用到海量智能体以及更复
杂的环境中是未来研究的重要方向.其次,现在普遍的多智
能体强化学习都是基于马尔可夫随机过程环境的,它给许多
模型提供了一个简单明确的数学框架,但是现实环境中很多
场景和问题是非马尔可夫的,在大部分多智能体场景中,其他
智能体的行为是不可预测的.上文提到的Shalev等[１４]和Xi
等[１６]提出的MARL新算法都不基于马尔可夫假设.因此,
在没有马尔可夫假设的情况下建模实现MARL,需要进行进
一步的研究与探索.最后,无监督的生成模型、计算图模型、
注意力机制等其他机器学习方法与MARL的结合可以解决
许多实际应用的问题.迁移学习也将被更多地应用到
MARL方法中,以缓解真实任务场景中训练数据缺乏的问
题.借助云服务器端的分布式的多智能体协同学习也是未来
一个重要的方向.
结束语　本文对多智能体强化学习进行了简要分析,包
括MARL的基本背景、经典算法、研究进展以及实际应用.
在许多现实问题和领域中,多智能体强化学习都展现出了巨
大的潜力,研究成果层出不穷,各种算法不断涌现.本文首先
介绍了多智能体强化学习的理论背景和经典算法,包括马尔
可夫框架和随机博弈模型等;然后主要从算法的可扩展性方
面详细综述了近期多智能体强化学习算法的创新和改进;最
后对多智能体强化学习的实际应用和未来前景展开讨论.多
智能体强化学习是一个新兴的、活跃的、快速扩展的研究领
域,还有很多问题值得进一步探索和研究.随着MARL理论

--- 分割段 25 ---
一切”这一目标终将被实现.
[１]ZHAOZH,GAOY,LUOB,etal．ReinforcementLearning
TechnologyinMultiＧAgentSystem[J]．ComputerScience,
２００４,３１(３):２３Ｇ２７．(inChinese)
赵志宏,高阳,骆斌,等．多Agent系统中强化学习的研究现状和
[２]GAOY,CHENSF,LUX．ResearchonReinforcementLearＧ
ningTechnology:AReview[J]．ACTAAUTOMATICASINIＧ
CA,２００４,３０(１):８６Ｇ１００．(inChinese)
高阳,陈世福,陆鑫．强化学习研究综述[J]．自动化学报,２００４,
３０(１):８６Ｇ１００．
[３]LIUQ,ZHAIJW,ZHANGZC,etal．ASurveyonDeepReinＧ
forcementLearning[J]．ChineseJournalofComputers,２０１８,
４０(１):１Ｇ２７．(inChinese)
刘全,翟建伟,章宗长,等．深度强化学习综述[J]．计算机学报,
２０１８,４０(１):１Ｇ２７．
[４]YANGWC,ZHANGL．MultiＧagentreinforcementlearning
basedtrafficsignalcontrolforintegratedurbannetwork:survey
ofstateofart[J]．ApplicationResearchofComputers,２０１８,
３５(６):１３Ｇ１８．(inChinese)
杨文臣,张轮．多智能体强化学习在城市交通网络信号控制方法
中的应用综述[J]．计算机应用研究,２０１８,３５(６):１３Ｇ１８．

--- 分割段 26 ---
３５(６):１３Ｇ１８．(inChinese)
杨文臣,张轮．多智能体强化学习在城市交通网络信号控制方法
中的应用综述[J]．计算机应用研究,２０１８,３５(６):１３Ｇ１８．
[５]ZHANGWX,MAL,WANGXD．Reinforcementlearningfor
eventＧtriggeredmultiＧagentsystems[J]．CAAITransactionson
IntelligentSystems,２０１７,１２(１):８２Ｇ８７．(inChinese)
张文旭,马磊,王晓东．基于事件驱动的多智能体强化学习研究
[J]．智能系统学报,２０１７,１２(１):８２Ｇ８７．
[６]XIL,CHENJF,HUANGYH,etal．Smartgenerationcontrol
basedonmultiＧagentreinforcementlearningwiththeideaofthe
timetunnel[J]．ScientiaSinica,２０１８,４８(４):４４１Ｇ４５６．(inChiＧ
nese)
席磊,陈建峰,黄悦华,等．基于具有时间隧道思想的多智能体强
化学习的智能发电控制方法[J]．中国科学:技术科学,２０１８,
４８(４):４４１Ｇ４５６．
[７]LITTMANML．MarkovgamesasaframeworkformultiＧagent
reinforcementlearning[M]．NewBrunswick:MachineLearning
Proceedings,１９９４:１５７Ｇ１６３．
[８]ZHAOXY,DINGSF．ResearchonDeepReinforcementLearＧ
ning[J]．ComputerScience,２０１８,４５(７):１Ｇ６．(inChinese)
４５(７):１Ｇ６．

--- 分割段 27 ---
ning[J]．ComputerScience,２０１８,４５(７):１Ｇ６．(inChinese)
４５(７):１Ｇ６．
[９]GUS,HOLLYE,LILLICRAPT,etal．Deepreinforcement
learningforroboticmanipulationwithasynchronousoffＧpolicy
updates[C]∥IEEEInternationalConferenceonRoboticsand
Automation．Singapore:IEEEPress,２０１７:３３８９Ｇ３３９６．
[１０]FOERSTERJ,ASSAELI,DEFREITASN,etal．Learningto
communicatewithdeepmultiＧagentreinforcementlearning[C]∥
AdvancesinNeuralInformationProcessingSystems．Spain:
NIPSPress,２０１６:２１３７Ｇ２１４５．
[１１]LOWER,WUY,etal．MultiＧagentactorＧcriticformixedcooＧ
perativeＧcompetitiveenvironments[C]∥AdvancesinNeuralInＧ
formationProcessingSystems．LosAngeles:NIPSPress,２０１７:
６３７９Ｇ６３９０．[１２]LANCTOTM,ZAMBALDIV,GRUSLYSA,etal．Aunified
gameＧtheoreticapproachtomultiＧagentreinforcementlearning
[C]∥AdvancesinNeuralInformationProcessingSystems．Los

--- 分割段 28 ---
[C]∥AdvancesinNeuralInformationProcessingSystems．Los
Angeles:NIPSPress,２０１７:４１９０Ｇ４２０３．
[１３]LEIBOJ,ZAMBALDIV,LANCTOTM,etal．MultiＧagentreinＧ
forcementlearninginsequentialsocialdilemmas[C]∥ProceeＧ
dingsofthe１６thConferenceonAutonomousAgentsandMultiＧ
agentSystems．Singapore:AAMASPress,２０１７:４６４Ｇ４７３．
[１４]SHALEVＧSHWARTZS,SHAMMAHS,SHASHUAA．Safe,
multiＧagent,reinforcementlearningforautonomousdriving[J/
OL]．https://arxiv．org/abs/１６１０．０３２９５．
[１５]JINJ,SONGC,LIH,etal．RealＧTimeBiddingwithMultiＧAＧ
gentReinforcementLearninginDisplayAdvertising[J/OL]．htＧ
tps://arxiv．org/abs/１８０２．０９７５６．
[１６]XIL,CHENJ,HUANGY,etal．Smartgenerationcontrol
basedonmultiＧagentreinforcementlearningwiththeideaofthe
timetunnel[J]．Energy,２０１８,１５３:９７７Ｇ９８７．
[１７]PEROLATJ,LEIBOJZ,ZAMBALDIV,etal．AmultiＧagent

--- 分割段 29 ---
timetunnel[J]．Energy,２０１８,１５３:９７７Ｇ９８７．
[１７]PEROLATJ,LEIBOJZ,ZAMBALDIV,etal．AmultiＧagent
reinforcementlearningmodelofcommonＧpoolresourceapproＧ
priation[C]∥AdvancesinNeuralInformationProcessingSysＧ
tems．LosAngeles:NIPSPress,２０１７:３６４３Ｇ３６５２．
[１８]SUTTONR．Introduction:Thechallengeofreinforcement
learning[M]．Springer,Boston,MA:ReinforcementLearning,
１９９２:１Ｇ３．
[１９]BUSONIUL,BABUŠKAR,DESCHUTTERB．MultiＧagent
reinforcementlearning:Anoverview[J]．InnovationsinmultiＧ
agentsystemsandapplicationsＧ１,２０１０,３１０:１８３Ｇ２２１．
[２０]WATKINSC,DAYANP．QＧlearning[J]．MachineLearning,
１９９２,８(３/４):２７９Ｇ２９２．
[２１]LITTMANM．ValueＧfunctionreinforcementlearninginMarkov
games[J]．CognitiveSystemsResearch,２００１,２(１):５５Ｇ６６．
[２２]LAUERM,RIEDMILLERM．AnAlgorithmforDistributed
ReinforcementLearninginCooperativeMultiＧAgentSystems

--- 分割段 30 ---
ReinforcementLearninginCooperativeMultiＧAgentSystems
[C]∥SeventeenthInternationalConferenceonMachineLearＧ
ning．Stanford:MorganKaufmannPress,２０００:５３５Ｇ５４２．
[２３]GREENWALDA,HALLK,SERRANOR．CorrelatedQＧlearＧ
ning[C]∥ICML．Washington:ICMLPress,２００３:２４２Ｇ２４９．
[２４]KONONENV．Asymmetricmultiagentreinforcementlearning
[C]∥InternationalConferenceonIntelligentAgentTechnoloＧ
gy．Canada:IEEEPress,２００３:３３６Ｇ３４２．
[２５]HUJ,WELLMANM．Multiagentreinforcementlearning:theoＧ
reticalframeworkandanalgorithm[C]∥ICML．Wisconsin:ICＧ
MLPress,１９９８:２４２Ｇ２５０．
[２６]MNIHV,KAVUKCUOGLUK,SILVERD,etal．PlayingAtari
withDeepReinforcementLearning[C]∥ProceedingsofWorkＧ
shopsatthe２６thNeuralInformationProcessingSystems２０１３．
LakeTahoe,USA:NIPSPress,２０１３:２０１Ｇ２２０．
[２７]VANHASSELTH,GUEZA,SILVERD．DeepReinforcement

--- 分割段 31 ---
LakeTahoe,USA:NIPSPress,２０１３:２０１Ｇ２２０．
[２７]VANHASSELTH,GUEZA,SILVERD．DeepReinforcement
LearningwithDoubleQＧLearning[C]∥AAAI．Arizona:AAAI
Press,２０１６:５．
[２８]SCHAULT,QUANJ,ANTONOGLOUI,etal．PrioritizedexＧ
periencereplay[C]∥proceedingsofthe４thInternationalConＧ
ferenceonLearningRepresentations．SanJuan,PuertoRico:
ICLRPress,２０１６:３２２Ｇ３５５．７ 第８期 杜　威,等:多智能体强化学习综述

--- 分割段 32 ---
[２９]OSBANDI,VANROYB,WENZ．GeneralizationandexploraＧ
tionviarandomizedvaluefunctions[J]．Proceedingsofthe３３rd
InternationalConferenceonInternationalConferenceonMaＧ
chineLearning,２０１４,４８(１):２３７７Ｇ２３８６．
[３０]MUNOSR,STEPLETONT,HARUTYUNYANA,etal．Safe
andefficientoffＧpolicyreinforcementlearning[C]∥Advancesin
NeuralInformationProcessingSystems．Spain:NIPSPress,
２０１６:１０５４Ｇ１０６２．
[３１]FRANÇOISＧLAVETV,FONTENEAUR,ERNSTD．Howto
discountdeepreinforcementlearning:Towardsnewdynamic
strategies[C]∥ProceedingsoftheWorkshopsattheAdvances
inNeuralInformationProcessingSystems．Montreal,Canada:
NIPSPress,２０１５:１１０７Ｇ１１６０．
[３２]LILLICRAPTP,HUNTJJ,PRITZELA,etal．Continuous
controlwithdeepreinforcementlearning:U．S．PatentApplicaＧ
tion１５/２１７,７５８[P]．２０１７Ｇ１Ｇ２６．
[３３]MNIHV,BADIAAP,MIRZAM,etal．Asynchronousmethods

--- 分割段 33 ---
tion１５/２１７,７５８[P]．２０１７Ｇ１Ｇ２６．
[３３]MNIHV,BADIAAP,MIRZAM,etal．Asynchronousmethods
fordeepreinforcementlearning[C]∥InternationalConference
onMachineLearning．NewYorkCity:ICMLpress,２０１６:１９２８Ｇ
１９３７．
[３４]SCHULMANJ,WOLSKIF,DHARIWALP,etal．Proximal
policyoptimizationalgorithms[J/OL]．https://arxiv．org/abs/
１７０７．０６３４７．
[３５]HEESSN,SRIRAMS,LEMMONJ,etal．EmergenceoflocoＧ
motionbehaviorsinrichenvironments[J/OL]．https://arxiv．
org/abs/１７０７．０２２８６．
[３６]FOERSTERJ,NARDELLIN,FARQUHARG,etal．StabiliＧ
zingexperiencereplayfordeepmultiＧagentreinforcementlearＧ
ning[J]．InternationalConferenceonMachineLearning,２０１７,
７０(３):１１４６Ｇ１１５５．
[３７]CIOSEKK,WHITESONS．Offer:OffenvironmentreinforceＧ
mentlearning[J]．AAAIConferenceonArtificialIntelligence,
２０１７．
[３８]TESAUROG．ExtendingqＧlearningtogeneraladaptivemultiＧaＧ

--- 分割段 34 ---
２０１７．
[３８]TESAUROG．ExtendingqＧlearningtogeneraladaptivemultiＧaＧ
gentsystems[J]．AdvancesinNeuralInformationProcessing
Systems,２００４,１６(４):８７１Ｇ８７８．
[３９]TANM．MultiＧAgentReinforcementLearning:Independentvs．
CooperativeAgents[C]∥ProceedingsoftheTenthInternational
ConferenceonMachineLearning．MA,USA:ICMLPress,１９９３:
３３０Ｇ３３７．
[４０]SHOHAMY,LEYTONK．MultiagentSystems:Algorithmic,
GameＧTheoretic,andLogicalFoundations[M]．NewYork:CamＧ
bridgeUniversityPress,２００９．
[４１]ZAWADZKIE,LIPSONA,LEYTONK．Empiricallyevaluating
multiagentlearningalgorithms[J/OL]．https://arxiv．org/abs/
１４０１．８０７４．
[４２]YANGY,LUOR,LIM,etal．MeanFieldMultiＧAgentReinＧ
forcementLearning[J/OL]．https://arxiv．org/abs/１８０２．
０５４３８．
[４３]PALMERG,TUYLSK,BLOEMBERGEND,etal．Lenient

--- 分割段 35 ---
０５４３８．
[４３]PALMERG,TUYLSK,BLOEMBERGEND,etal．Lenient
multiＧagentdeepreinforcementlearning[C]∥Proceedingsofthe１７thInternationalConferenceonAutonomousAgentsand
MultiＧagentSystems．Swede:AAMASpress,２０１８:４４３Ｇ４５１．
[４４]ZHENGY,MENGZ,HAOJ,etal．WeightedDoubleDeep
MultiagentReinforcementLearninginStochasticCooperative
Environments[C]∥PacificRimInternationalConferenceon
ArtificialIntelligence．Springer,Cham:PRICAIpress,２０１８:４２１Ｇ
[４５]TAMPUUA,MATIISENT,KODELJAD,etal．Multiagent
cooperationandcompetitionwithdeepreinforcementlearning
[J]．PlusOne,２０１７,１２(４):e０１７２３９５．
[４６]SONGJ,RENH,SADIGHD,etal．MultiＧagentgenerativeadＧ
versarialimitationlearning[J/OL]．https://arxiv．org/abs/
１８０７．０９９３６．
[４７]WAIHT,YANGZ,WANGZ,etal．MultiＧAgentReinforceＧ
mentLearningviaDoubleAveragingPrimalＧDualOptimization

--- 分割段 36 ---
mentLearningviaDoubleAveragingPrimalＧDualOptimization
[J/OL]．https://arxiv．org/abs/１８０６．００８７７．
[４８]ABOUHEAFM,GUEAIEBW．MultiＧagentreinforcement
learningapproachbasedonreducedvaluefunctionapproximaＧ
tions[C]∥２０１７IEEEInternationalSymposiumonRoboticsand
IntelligentSensors(IRIS)．Canada:IEEEPress,２０１７:１１１Ｇ１１６．
[４９]QIS,ZHUSC．IntentＧawareMultiＧagentReinforcementLearＧ
ning[J/OL]．https://arxiv．org/abs/１８０３．０２０１８．
[５０]RAILEANUR,DENTONE,SZLAMA,etal．ModelingOthers
usingOneselfinMultiＧAgentReinforcementLearning[J/OL]．
https://arxiv．org/abs/１８０２．０９６４０．
[５１]RABINOWITZN,PERBETF,SONGH,etal．MachineTheory
ofMind[J/OL]．https://arxiv．org/abs/１８０２．０７７４０．
[５２]OMIDSHAFIEIS,KIMD,LIUM,etal．LearningtoTeachin
CooperativeMultiagentReinforcementLearning[J/OL]．htＧ
tps://arxiv．org/abs/１８０５．０７８３０．

--- 分割段 37 ---
CooperativeMultiagentReinforcementLearning[J/OL]．htＧ
tps://arxiv．org/abs/１８０５．０７８３０．
[５３]GUS,LILLICRAPT,SUTSKEVERI,etal．Continuousdeep
qＧlearningwithmodelＧbasedacceleration[C]∥International
ConferenceonMachineLearning．NewYorkCity:ICMLPress,
２０１６:２８２９Ｇ２８３８．
[５４]DUANY,CHENX,HOUTHOOFTR,etal．Benchmarking
deepreinforcementlearningforcontinuouscontrol[C]∥InterＧ
nationalConferenceonMachineLearning．NewYorkCity:ICＧ
MLPress,２０１６:１３２９Ｇ１３３８．
[５５]KOFINASP,DOUNISAI,VOUROSGA．FuzzyQＧLearning
formultiＧagentdecentralizedenergymanagementinmicrogrids
[J]．AppliedEnergy,２０１８,２１９(３):５３Ｇ６７．
[５６]CHENW,ZHOUK,CHENC．RealＧtimebusholdingcontrolon
atransitcorridorbasedonmultiＧagentreinforcementlearning
[C]∥２０１６IEEE１９thInternationalConferenceonIntelligent
TransportationSystems(ITSC)．Brazil:IEEEPress,２０１６:１００Ｇ

--- 分割段 38 ---
TransportationSystems(ITSC)．Brazil:IEEEPress,２０１６:１００Ｇ
[５７]VIDHATEDA,KULKARNIP．CooperativemultiＧagentreinＧ
forcementlearningmodels(CMRLM)forintelligenttrafficconＧ
trol[C]∥２０１７１stInternationalConferenceonIntelligentSysＧ
temsandInformationManagement(ICISIM)．India:IEEE
