{
  "filename": "DeepSeek_R1.pdf",
  "total_chunks": 175,
  "total_pages": 22,
  "loading_method": "unstructured",
  "loading_strategy": "fast",
  "chunking_strategy": "by_title",
  "chunking_method": "loaded",
  "timestamp": "2025-02-14T18:52:52.297218",
  "chunks": [
    {
      "content": "5 2 0 2\n\nn a J\n\n2 2\n\n] L C . s c [\n\n1 v 8 4 9 2 1 . 1 0 5 2 : v i X r a\n\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n\nDeepSeek-AI\n\nresearch@deepseek.com\n\nAbstract",
      "metadata": {
        "chunk_id": 1,
        "page_number": 1,
        "page_range": "1",
        "word_count": 46,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Text object at 0x7fb3f2ab94b0>, <unstructured.documents.elements.Title object at 0x7fb3f2ab9b40>, <unstructured.documents.elements.Text object at 0x7fb3f2ab9db0>, <unstructured.documents.elements.Title object at 0x7fb3f2abb5e0>, <unstructured.documents.elements.Text object at 0x7fb3f2ab8e50>, <unstructured.documents.elements.Title object at 0x7fb3f2ab9630>, <unstructured.documents.elements.Title object at 0x7fb3f2ab9b10>, <unstructured.documents.elements.EmailAddress object at 0x7fb3f2ab8e80>, <unstructured.documents.elements.Title object at 0x7fb3f2ab8b50>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "e3e727faab194269ce441eddb0a08539",
        "category": "CompositeElement"
      }
    },
    {
      "content": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super- vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further",
      "metadata": {
        "chunk_id": 2,
        "page_number": 1,
        "page_range": "1",
        "word_count": 60,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2abb490>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "51086b3645e93f58ff32fa7bc74ce88e",
        "category": "CompositeElement"
      }
    },
    {
      "content": "enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek- R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
      "metadata": {
        "chunk_id": 3,
        "page_number": 1,
        "page_range": "1",
        "word_count": 52,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2abb490>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "9cf1b24d9334492095997d0f1572e185",
        "category": "CompositeElement"
      }
    },
    {
      "content": "DeepSeek-R1-32B\n\n40\n\n79.896.371.597.390.849.279.296.675.796.491.848.972.690.662.194.387.436.863.693.460.090.085.241.639.258.759.190.288.542.0\n\nOpenAI-o1-mini\n\nDeepSeek-V3\n\n80\n\n20\n\nOpenAI-o1-1217\n\nDeepSeek-R1\n\n60\n\n100Accuracy / Percentile (%)\n\n0\n\nAIME 2024(Pass@1)Codeforces(Percentile)GPQA Diamond(Pass@1)MATH-500(Pass@1)MMLU(Pass@1)SWE-bench Verified(Resolved)\n\nFigure 1 | Benchmark performance of DeepSeek-R1.\n\nContents\n\n1\n\nIntroduction",
      "metadata": {
        "chunk_id": 4,
        "page_number": 1,
        "page_range": "1",
        "word_count": 29,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3f04a7bb0>, <unstructured.documents.elements.Text object at 0x7fb3f04a4d00>, <unstructured.documents.elements.Text object at 0x7fb3f04a7eb0>, <unstructured.documents.elements.Title object at 0x7fb3f04a41c0>, <unstructured.documents.elements.Title object at 0x7fb3f04a75e0>, <unstructured.documents.elements.Text object at 0x7fb3f04a7190>, <unstructured.documents.elements.Text object at 0x7fb3f04a4ca0>, <unstructured.documents.elements.Title object at 0x7fb3f04a4e80>, <unstructured.documents.elements.Title object at 0x7fb3f04a7e20>, <unstructured.documents.elements.Text object at 0x7fb3f04a5690>, <unstructured.documents.elements.Title object at 0x7fb3f04a5ab0>, <unstructured.documents.elements.Text object at 0x7fb3f04a7ee0>, <unstructured.documents.elements.Title object at 0x7fb3f04a6ce0>, <unstructured.documents.elements.Title object at 0x7fb3f04a4fd0>, <unstructured.documents.elements.Title object at 0x7fb3f406ebf0>, <unstructured.documents.elements.Text object at 0x7fb3f406dc00>, <unstructured.documents.elements.Title object at 0x7fb3f406dba0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "3b515fa34113c455bb09374ae1a08687",
        "category": "CompositeElement"
      }
    },
    {
      "content": "1.1 Contributions .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n1.2\n\nSummary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2 Approach\n\n2.1 Overview .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "metadata": {
        "chunk_id": 5,
        "page_number": 2,
        "page_range": "2",
        "word_count": 121,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3f406e380>, <unstructured.documents.elements.Text object at 0x7fb3f406ecb0>, <unstructured.documents.elements.Text object at 0x7fb3dbbcb280>, <unstructured.documents.elements.Text object at 0x7fb3dbbcb0a0>, <unstructured.documents.elements.Text object at 0x7fb3dbbc8fa0>, <unstructured.documents.elements.Text object at 0x7fb3dbbc84f0>, <unstructured.documents.elements.Text object at 0x7fb3dbbcb5e0>, <unstructured.documents.elements.Text object at 0x7fb3dbbc8820>, <unstructured.documents.elements.Text object at 0x7fb3dbbcace0>, <unstructured.documents.elements.Text object at 0x7fb3dbbca620>, <unstructured.documents.elements.Title object at 0x7fb3dbbc9d80>, <unstructured.documents.elements.Title object at 0x7fb3dbbcb220>, <unstructured.documents.elements.Text object at 0x7fb3dbbcbf10>, <unstructured.documents.elements.Text object at 0x7fb3dbbc9b10>, <unstructured.documents.elements.Text object at 0x7fb3dbbc8370>, <unstructured.documents.elements.Text object at 0x7fb3dbbcbb80>, <unstructured.documents.elements.Text object at 0x7fb3dbbcae60>, <unstructured.documents.elements.Text object at 0x7fb3dbbc9f00>, <unstructured.documents.elements.Text object at 0x7fb3dbbcbe20>, <unstructured.documents.elements.Text object at 0x7fb3dbbc8910>, <unstructured.documents.elements.Text object at 0x7fb3dbbc8be0>, <unstructured.documents.elements.Text object at 0x7fb3dbbc80d0>, <unstructured.documents.elements.Text object at 0x7fb3dbbc8f40>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "1f8dcfbc0f729e9ad8c32c0a858589b7",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\n\n. . . . . . . . . .\n\n2.2.1 Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . .\n\n2.2.2 Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2.2.3 Training Template\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2.2.4\n\nPerformance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6",
      "metadata": {
        "chunk_id": 6,
        "page_number": 2,
        "page_range": "2",
        "word_count": 124,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbbca1d0>, <unstructured.documents.elements.Text object at 0x7fb3dbbcb880>, <unstructured.documents.elements.Text object at 0x7fb3dbbc8430>, <unstructured.documents.elements.Text object at 0x7fb3dbbc9db0>, <unstructured.documents.elements.Title object at 0x7fb3dbbc92d0>, <unstructured.documents.elements.Text object at 0x7fb3dbbcbbb0>, <unstructured.documents.elements.Text object at 0x7fb3db9e37c0>, <unstructured.documents.elements.Title object at 0x7fb3db9e2a40>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "9ed5c34db987c0c6436cde21b7310dec",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2.3 DeepSeek-R1: Reinforcement Learning with Cold Start\n\n. . . . . . . . . . . . . . .",
      "metadata": {
        "chunk_id": 7,
        "page_number": 2,
        "page_range": "2",
        "word_count": 22,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3db9e2d40>, <unstructured.documents.elements.Text object at 0x7fb3dbbc90c0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "ccd4eadf6862fede670fb4beef4e7983",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2.3.1 Cold Start\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2.3.2 Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . .\n\n2.3.3 Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . .\n\n2.3.4 Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . .\n\n2.4 Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . .\n\n3 Experiment",
      "metadata": {
        "chunk_id": 8,
        "page_number": 2,
        "page_range": "2",
        "word_count": 127,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3db9e32b0>, <unstructured.documents.elements.Text object at 0x7fb3db9e0e80>, <unstructured.documents.elements.Text object at 0x7fb3db9e3a60>, <unstructured.documents.elements.Text object at 0x7fb3db9e3700>, <unstructured.documents.elements.Text object at 0x7fb3db9e1510>, <unstructured.documents.elements.Text object at 0x7fb3db9e13f0>, <unstructured.documents.elements.Text object at 0x7fb3db9e2260>, <unstructured.documents.elements.Text object at 0x7fb3db9e36d0>, <unstructured.documents.elements.Text object at 0x7fb3db9e08e0>, <unstructured.documents.elements.Text object at 0x7fb3db9e0880>, <unstructured.documents.elements.Text object at 0x7fb3db9e2440>, <unstructured.documents.elements.Text object at 0x7fb3db9e1360>, <unstructured.documents.elements.Text object at 0x7fb3db9e3430>, <unstructured.documents.elements.Title object at 0x7fb3db9e3970>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "fe757d2d5f39801d7f63edb621b649bc",
        "category": "CompositeElement"
      }
    },
    {
      "content": "3.1 DeepSeek-R1 Evaluation .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n3.2 Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n4 Discussion\n\n4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .\n\n4.2 Unsuccessful Attempts .\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n5 Conclusion, Limitations, and Future Work",
      "metadata": {
        "chunk_id": 9,
        "page_number": 2,
        "page_range": "2",
        "word_count": 145,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3db9e0760>, <unstructured.documents.elements.Text object at 0x7fb3db9e1b70>, <unstructured.documents.elements.Text object at 0x7fb3db9e1ae0>, <unstructured.documents.elements.Title object at 0x7fb3db9e2b00>, <unstructured.documents.elements.Text object at 0x7fb3db9e21d0>, <unstructured.documents.elements.Title object at 0x7fb3db9e39a0>, <unstructured.documents.elements.Text object at 0x7fb3db9e1cf0>, <unstructured.documents.elements.Text object at 0x7fb3db9e2ce0>, <unstructured.documents.elements.Title object at 0x7fb3db9e0250>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "beb0c4af55ae33dd70de19c12f3c54d9",
        "category": "CompositeElement"
      }
    },
    {
      "content": "A Contributions and Acknowledgments\n\n2\n\n3\n\n4\n\n4\n\n5\n\n5\n\n5\n\n5\n\n6\n\n6\n\n9\n\n9\n\n10\n\n10\n\n11\n\n11\n\n11\n\n13\n\n14\n\n14\n\n14\n\n15\n\n16\n\n20\n\n1. Introduction\n\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI).",
      "metadata": {
        "chunk_id": 10,
        "page_number": 2,
        "page_range": "2",
        "word_count": 59,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3db9e2740>, <unstructured.documents.elements.Footer object at 0x7fb3db9e3be0>, <unstructured.documents.elements.Text object at 0x7fb3dbbc8490>, <unstructured.documents.elements.Text object at 0x7fb3dbbc89a0>, <unstructured.documents.elements.Text object at 0x7fb3dbbcb760>, <unstructured.documents.elements.Text object at 0x7fb3dbbca020>, <unstructured.documents.elements.Text object at 0x7fb3dbbc9ff0>, <unstructured.documents.elements.Text object at 0x7fb3dbbcbfd0>, <unstructured.documents.elements.Text object at 0x7fb3dbbca140>, <unstructured.documents.elements.Text object at 0x7fb3dbbc8df0>, <unstructured.documents.elements.Text object at 0x7fb3dbbc9360>, <unstructured.documents.elements.Text object at 0x7fb3db9e2110>, <unstructured.documents.elements.Text object at 0x7fb3db9e0f10>, <unstructured.documents.elements.Text object at 0x7fb3db9e1990>, <unstructured.documents.elements.Text object at 0x7fb3db9e3df0>, <unstructured.documents.elements.Text object at 0x7fb3db9e2620>, <unstructured.documents.elements.Text object at 0x7fb3db9e2980>, <unstructured.documents.elements.Text object at 0x7fb3db9e2f20>, <unstructured.documents.elements.Text object at 0x7fb3db9e21a0>, <unstructured.documents.elements.Text object at 0x7fb3db9e0a30>, <unstructured.documents.elements.Text object at 0x7fb3db9e00a0>, <unstructured.documents.elements.Text object at 0x7fb3db9e2080>, <unstructured.documents.elements.Text object at 0x7fb3db9e3fd0>, <unstructured.documents.elements.Text object at 0x7fb3db9e09a0>, <unstructured.documents.elements.Text object at 0x7fb3db9e28c0>, <unstructured.documents.elements.ListItem object at 0x7fb3f04a4be0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f406e530>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "063fc0441242de87bdb55564c19f8a08",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of- Thought reasoning process.",
      "metadata": {
        "chunk_id": 11,
        "page_number": 3,
        "page_range": "3",
        "word_count": 70,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f04a7af0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "59671194e7bb8bff98172c668db4e4a6",
        "category": "CompositeElement"
      }
    },
    {
      "content": "This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search",
      "metadata": {
        "chunk_id": 12,
        "page_number": 3,
        "page_range": "3",
        "word_count": 70,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f04a7af0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "298420f21138c252b91e8f514bcdc3af",
        "category": "CompositeElement"
      }
    },
    {
      "content": "and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI’s o1 series models.",
      "metadata": {
        "chunk_id": 13,
        "page_number": 3,
        "page_range": "3",
        "word_count": 31,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f04a7af0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "8eded660d218a965c875fdb7f732b069",
        "category": "CompositeElement"
      }
    },
    {
      "content": "In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally",
      "metadata": {
        "chunk_id": 14,
        "page_number": 3,
        "page_range": "3",
        "word_count": 74,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f04a5960>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "86738cb0284228818edcad00c866b119",
        "category": "CompositeElement"
      }
    },
    {
      "content": "emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.",
      "metadata": {
        "chunk_id": 15,
        "page_number": 3,
        "page_range": "3",
        "word_count": 48,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f04a5960>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "7ded36fcd1b8f3eb6282619f3a6866e2",
        "category": "CompositeElement"
      }
    },
    {
      "content": "However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1- Zero. Upon nearing convergence in the RL",
      "metadata": {
        "chunk_id": 16,
        "page_number": 3,
        "page_range": "3",
        "word_count": 65,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f04a5cc0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "b6d899dea14b0c8e3d99077fb65cda39",
        "category": "CompositeElement"
      }
    },
    {
      "content": "process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.",
      "metadata": {
        "chunk_id": 17,
        "page_number": 3,
        "page_range": "3",
        "word_count": 72,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f04a5cc0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "547e43b1407ffadc4c80aa4aeadca2e2",
        "category": "CompositeElement"
      }
    },
    {
      "content": "We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5- 32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru- cial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen,",
      "metadata": {
        "chunk_id": 18,
        "page_number": 3,
        "page_range": "3",
        "word_count": 68,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f04a7700>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "e80fb0c466e6c572eb257008f27a01d5",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.",
      "metadata": {
        "chunk_id": 19,
        "page_number": 3,
        "page_range": "3",
        "word_count": 23,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f04a7700>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "8163fcce6195280555090c6e90ec4cb4",
        "category": "CompositeElement"
      }
    },
    {
      "content": "3\n\n1.1. Contributions\n\nPost-Training: Large-Scale Reinforcement Learning on the Base Model",
      "metadata": {
        "chunk_id": 20,
        "page_number": 3,
        "page_range": "3",
        "word_count": 11,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Footer object at 0x7fb3f04a5de0>, <unstructured.documents.elements.Title object at 0x7fb3f406dcc0>, <unstructured.documents.elements.Title object at 0x7fb3f406cfd0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "c88bef4b401f117c269be3d89532ec43",
        "category": "CompositeElement"
      }
    },
    {
      "content": "We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek- R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning",
      "metadata": {
        "chunk_id": 21,
        "page_number": 4,
        "page_range": "4",
        "word_count": 68,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3f406cc40>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "77129e0fcf32306582f292fb6b8d1f9a",
        "category": "CompositeElement"
      }
    },
    {
      "content": "capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.",
      "metadata": {
        "chunk_id": 22,
        "page_number": 4,
        "page_range": "4",
        "word_count": 25,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3f406cc40>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "d6b6304bb40b8da0da89a006f25fd81c",
        "category": "CompositeElement"
      }
    },
    {
      "content": "We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human pref- erences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models.",
      "metadata": {
        "chunk_id": 23,
        "page_number": 4,
        "page_range": "4",
        "word_count": 55,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3f406d930>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "7c0d43190c681e30f831698fdae067ff",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Distillation: Smaller Models Can Be Powerful Too\n\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.",
      "metadata": {
        "chunk_id": 24,
        "page_number": 4,
        "page_range": "4",
        "word_count": 59,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3f406dbd0>, <unstructured.documents.elements.ListItem object at 0x7fb3f406d210>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "f2e4ae285d76970e7c32675af70f5718",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek- R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi- tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform",
      "metadata": {
        "chunk_id": 25,
        "page_number": 4,
        "page_range": "4",
        "word_count": 63,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3f406d9f0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "90e8ee2110d1bb663e4ecc4ea3176670",
        "category": "CompositeElement"
      }
    },
    {
      "content": "previous open- source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.",
      "metadata": {
        "chunk_id": 26,
        "page_number": 4,
        "page_range": "4",
        "word_count": 29,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3f406d9f0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "33a3bf8e15dc8aec57b063ab8aedc109",
        "category": "CompositeElement"
      }
    },
    {
      "content": "1.2. Summary of Evaluation Results",
      "metadata": {
        "chunk_id": 27,
        "page_number": 4,
        "page_range": "4",
        "word_count": 5,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3f04a4250>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "87d4da8ba977ca1cc7e98d8261219f99",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1",
      "metadata": {
        "chunk_id": 28,
        "page_number": 4,
        "page_range": "4",
        "word_count": 66,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3f04a79a0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "9408c5f24c063fe45c5d628d8fa43841",
        "category": "CompositeElement"
      }
    },
    {
      "content": "performs slightly better than DeepSeek-V3, which could help developers in real world tasks.",
      "metadata": {
        "chunk_id": 29,
        "page_number": 4,
        "page_range": "4",
        "word_count": 13,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3f04a79a0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "46d32387f587e6daf2c819b0cbd7d65e",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek- R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,",
      "metadata": {
        "chunk_id": 30,
        "page_number": 4,
        "page_range": "4",
        "word_count": 64,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3f4026260>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "d173531195b04d28facf34c52d463426",
        "category": "CompositeElement"
      }
    },
    {
      "content": "demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark. 4",
      "metadata": {
        "chunk_id": 31,
        "page_number": 4,
        "page_range": "4",
        "word_count": 20,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3f4026260>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "96a3ccfe88cdb922c7f15097b8c41efc",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are- naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3",
      "metadata": {
        "chunk_id": 32,
        "page_number": 5,
        "page_range": "5",
        "word_count": 61,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3db9e15d0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "b9af38dd3126a0b00915f63953b483ae",
        "category": "CompositeElement"
      }
    },
    {
      "content": "on long-context benchmarks.",
      "metadata": {
        "chunk_id": 33,
        "page_number": 5,
        "page_range": "5",
        "word_count": 3,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3db9e15d0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "8bf90d813756ace51c41e022aa0f7155",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2. Approach\n\n2.1. Overview",
      "metadata": {
        "chunk_id": 34,
        "page_number": 5,
        "page_range": "5",
        "word_count": 4,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3dbb1ee60>, <unstructured.documents.elements.Title object at 0x7fb3dbb1fa90>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "801863237a86d493d3b98b9ee400f9bf",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the",
      "metadata": {
        "chunk_id": 35,
        "page_number": 5,
        "page_range": "5",
        "word_count": 72,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbb1ff70>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "8135802b71a5c677e2ba5cd57cc3f41a",
        "category": "CompositeElement"
      }
    },
    {
      "content": "base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models.",
      "metadata": {
        "chunk_id": 36,
        "page_number": 5,
        "page_range": "5",
        "word_count": 35,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbb1ff70>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "8396b1635f69a18eec6309771d942297",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model",
      "metadata": {
        "chunk_id": 37,
        "page_number": 5,
        "page_range": "5",
        "word_count": 8,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbb1f9d0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "aba989ba4991c15c12859016d39ad276",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev- idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL",
      "metadata": {
        "chunk_id": 38,
        "page_number": 5,
        "page_range": "5",
        "word_count": 73,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbb1f670>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "bf43594050b71ecb00de8bb34fa3fb4d",
        "category": "CompositeElement"
      }
    },
    {
      "content": "algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights.",
      "metadata": {
        "chunk_id": 39,
        "page_number": 5,
        "page_range": "5",
        "word_count": 18,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbb1f670>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "ca6d4948cca2456f83a548e977ddf2c2",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2.2.1. Reinforcement Learning Algorithm",
      "metadata": {
        "chunk_id": 40,
        "page_number": 5,
        "page_range": "5",
        "word_count": 4,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbb1fdf0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "ee5b5189fea7f9078a029c3aef743aca",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old policy 𝜋𝜃𝑜𝑙𝑑 and then optimizes the policy model 𝜋𝜃 by maximizing the following objective:",
      "metadata": {
        "chunk_id": 41,
        "page_number": 5,
        "page_range": "5",
        "word_count": 82,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbb1f7c0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "97e82037a8f3208dfc48f9eafb5e09b3",
        "category": "CompositeElement"
      }
    },
    {
      "content": "J𝐺𝑅𝑃𝑂(𝜃) = E[𝑞 ∼ 𝑃(𝑄), {𝑜𝑖}𝐺 𝑖=1 (cid:18) 𝜋𝜃(𝑜𝑖|𝑞) (cid:18) 𝜋𝜃𝑜𝑙𝑑 (𝑜𝑖|𝑞)\n\n𝐺 ∑︁\n\n1 𝐺\n\nmin\n\n𝑖=1\n\n∼ 𝜋𝜃𝑜𝑙𝑑 (𝑂|𝑞)]\n\n𝐴𝑖, clip\n\n(cid:18) 𝜋𝜃(𝑜𝑖|𝑞) 𝜋𝜃𝑜𝑙𝑑 (𝑜𝑖|𝑞)\n\n, 1 − 𝜀, 1 + 𝜀\n\n(cid:19)\n\n𝐴𝑖\n\n(cid:19)\n\n− 𝛽D𝐾𝐿 (cid:0)𝜋𝜃||𝜋𝑟𝑒 𝑓 (cid:1)\n\nD𝐾𝐿 (cid:0)𝜋𝜃||𝜋𝑟𝑒 𝑓 (cid:1) =\n\n𝜋𝑟𝑒 𝑓 (𝑜𝑖|𝑞) 𝜋𝜃(𝑜𝑖|𝑞)\n\n− log\n\n𝜋𝑟𝑒 𝑓 (𝑜𝑖|𝑞) 𝜋𝜃(𝑜𝑖|𝑞)\n\n− 1,\n\nwhere 𝜀 and 𝛽 are hyper-parameters, and 𝐴𝑖 is the advantage, computed using a group of rewards {𝑟1, 𝑟2, . . . , 𝑟𝐺} corresponding to the outputs within each group:",
      "metadata": {
        "chunk_id": 42,
        "page_number": 5,
        "page_range": "5",
        "word_count": 90,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbb1ece0>, <unstructured.documents.elements.Text object at 0x7fb3dbb1df00>, <unstructured.documents.elements.Title object at 0x7fb3dbb1d990>, <unstructured.documents.elements.Title object at 0x7fb3db9e1b10>, <unstructured.documents.elements.Text object at 0x7fb3dbb1ce20>, <unstructured.documents.elements.Title object at 0x7fb3dbb1cc40>, <unstructured.documents.elements.Title object at 0x7fb3dbb1cb20>, <unstructured.documents.elements.Title object at 0x7fb3dbb1cc70>, <unstructured.documents.elements.Text object at 0x7fb3dbb1d060>, <unstructured.documents.elements.Text object at 0x7fb3dbb1cee0>, <unstructured.documents.elements.Title object at 0x7fb3dbb1d330>, <unstructured.documents.elements.Text object at 0x7fb3dbb1ee30>, <unstructured.documents.elements.Title object at 0x7fb3dbb1c8b0>, <unstructured.documents.elements.Title object at 0x7fb3dbb1fe20>, <unstructured.documents.elements.Title object at 0x7fb3dbb1ca90>, <unstructured.documents.elements.Title object at 0x7fb3dbb1db70>, <unstructured.documents.elements.Title object at 0x7fb3dbb1cfa0>, <unstructured.documents.elements.Text object at 0x7fb3dbb1f2b0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbb1c160>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "47451b838e97788083abd560c4cd19d0",
        "category": "CompositeElement"
      }
    },
    {
      "content": "𝐴𝑖 =\n\n𝑟𝑖 − m𝑒𝑎𝑛({𝑟1, 𝑟2, · · · , 𝑟𝐺}) s𝑡𝑑({𝑟1, 𝑟2, · · · , 𝑟𝐺})\n\n.\n\n5\n\n(cid:19)\n\n,\n\n(1)\n\n(2)\n\n(3)",
      "metadata": {
        "chunk_id": 43,
        "page_number": 5,
        "page_range": "5",
        "word_count": 25,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbb1c1f0>, <unstructured.documents.elements.Text object at 0x7fb3dbb1d720>, <unstructured.documents.elements.Text object at 0x7fb3dbb1e410>, <unstructured.documents.elements.Footer object at 0x7fb3dbb1e7a0>, <unstructured.documents.elements.Text object at 0x7fb3dbb1e2c0>, <unstructured.documents.elements.Text object at 0x7fb3dbb1c730>, <unstructured.documents.elements.Text object at 0x7fb3dbb1f340>, <unstructured.documents.elements.Text object at 0x7fb3dbb1cd30>, <unstructured.documents.elements.Text object at 0x7fb3dbb1c070>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "22487f4ebafb87dd3b5408532361498b",
        "category": "CompositeElement"
      }
    },
    {
      "content": "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant:",
      "metadata": {
        "chunk_id": 44,
        "page_number": 6,
        "page_range": "6",
        "word_count": 63,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f0417640>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "59fc5314e47ba73781c6a1c5ee529604",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Table 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning question during training.\n\n2.2.2. Reward Modeling\n\nThe reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:",
      "metadata": {
        "chunk_id": 45,
        "page_number": 6,
        "page_range": "6",
        "word_count": 53,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f0416e60>, <unstructured.documents.elements.Title object at 0x7fb3f0414c10>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f04165f0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "7cab7362d0e3b8179197b0b5222b08bf",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.",
      "metadata": {
        "chunk_id": 46,
        "page_number": 6,
        "page_range": "6",
        "word_count": 63,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3f0415420>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "7586e1bfd3daf7721d3e09beab390490",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’ tags.",
      "metadata": {
        "chunk_id": 47,
        "page_number": 6,
        "page_range": "6",
        "word_count": 29,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3f0417a00>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "4663242d08571516bdae5b7403a6238c",
        "category": "CompositeElement"
      }
    },
    {
      "content": "We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.\n\n2.2.3. Training Template",
      "metadata": {
        "chunk_id": 48,
        "page_number": 6,
        "page_range": "6",
        "word_count": 52,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f0415db0>, <unstructured.documents.elements.Title object at 0x7fb3f04170a0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "e61d96830b9c6d8cb857a7130a4e6c0c",
        "category": "CompositeElement"
      }
    },
    {
      "content": "To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biases—such as mandating reflective reasoning or promoting particular problem-solving strate- gies—to ensure that we can",
      "metadata": {
        "chunk_id": 49,
        "page_number": 6,
        "page_range": "6",
        "word_count": 68,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbb1fd60>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "de572119320e92fccd52a9491e0a43a9",
        "category": "CompositeElement"
      }
    },
    {
      "content": "accurately observe the model’s natural progression during the RL process.",
      "metadata": {
        "chunk_id": 50,
        "page_number": 6,
        "page_range": "6",
        "word_count": 10,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbb1fd60>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "e7f82137310a5ef2f825c5d0d7149872",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero",
      "metadata": {
        "chunk_id": 51,
        "page_number": 6,
        "page_range": "6",
        "word_count": 9,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3f0414a90>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "2c629bfa2cd33758bcb59e5a332347e6",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek- R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant",
      "metadata": {
        "chunk_id": 52,
        "page_number": 6,
        "page_range": "6",
        "word_count": 67,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f04161d0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "36eecb92ce30a7bcf1b2d2789b538a24",
        "category": "CompositeElement"
      }
    },
    {
      "content": "improvement highlights the efficacy of our RL algorithm in optimizing the model’s performance over time.",
      "metadata": {
        "chunk_id": 53,
        "page_number": 6,
        "page_range": "6",
        "word_count": 15,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f04161d0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "ce576bac5885079f7735d1f77fb48eb9",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers\n\n6\n\nModel\n\nAIME 2024\n\nMATH-500\n\nGPQA Diamond\n\nLiveCode Bench\n\nCodeForces\n\npass@1\n\ncons@64\n\npass@1\n\npass@1\n\npass@1\n\nrating\n\nOpenAI-o1-mini OpenAI-o1-0912\n\n63.6 74.4\n\n80.0 83.3\n\n90.0 94.8\n\n60.0 77.3\n\n53.8 63.4\n\n1820 1843",
      "metadata": {
        "chunk_id": 54,
        "page_number": 6,
        "page_range": "6",
        "word_count": 54,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f0417940>, <unstructured.documents.elements.Footer object at 0x7fb3f0414cd0>, <unstructured.documents.elements.Title object at 0x7fb3dbcef6a0>, <unstructured.documents.elements.Title object at 0x7fb3dbcef130>, <unstructured.documents.elements.Title object at 0x7fb3dbced6c0>, <unstructured.documents.elements.Title object at 0x7fb3dbcec160>, <unstructured.documents.elements.Title object at 0x7fb3dbcefb20>, <unstructured.documents.elements.Title object at 0x7fb3dbcec700>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbceca00>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbcee0e0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbced180>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbced4b0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbcee080>, <unstructured.documents.elements.Title object at 0x7fb3dbcee980>, <unstructured.documents.elements.Title object at 0x7fb3dbcec5e0>, <unstructured.documents.elements.Text object at 0x7fb3dbced420>, <unstructured.documents.elements.Text object at 0x7fb3dbcec4f0>, <unstructured.documents.elements.Text object at 0x7fb3dbbcaf20>, <unstructured.documents.elements.Text object at 0x7fb3dbcec940>, <unstructured.documents.elements.Text object at 0x7fb3dbcedcc0>, <unstructured.documents.elements.Text object at 0x7fb3dbcef940>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "f74a4f9f96ee143d607f8674a042ee93",
        "category": "CompositeElement"
      }
    },
    {
      "content": "DeepSeek-R1-Zero\n\n71.0\n\n86.7\n\n95.9\n\n73.3\n\n50.0\n\n1444\n\nTable 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks.\n\nFigure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.",
      "metadata": {
        "chunk_id": 55,
        "page_number": 7,
        "page_range": "7",
        "word_count": 47,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbcee590>, <unstructured.documents.elements.Text object at 0x7fb3dbbcba60>, <unstructured.documents.elements.Text object at 0x7fb3dbceebf0>, <unstructured.documents.elements.Text object at 0x7fb3dbcefb50>, <unstructured.documents.elements.Text object at 0x7fb3dbcef0a0>, <unstructured.documents.elements.Text object at 0x7fb3dbcec790>, <unstructured.documents.elements.Text object at 0x7fb3dbcef010>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbced060>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbcee290>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "cfaf1bf11e4afb6715d822b7e78aa807",
        "category": "CompositeElement"
      }
    },
    {
      "content": "DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek- R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance escalates from 71.0% to 86.7%, thereby",
      "metadata": {
        "chunk_id": 56,
        "page_number": 7,
        "page_range": "7",
        "word_count": 68,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbced330>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "f81704257c5ce6d1bdfe94f21735f1fa",
        "category": "CompositeElement"
      }
    },
    {
      "content": "exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.",
      "metadata": {
        "chunk_id": 57,
        "page_number": 7,
        "page_range": "7",
        "word_count": 34,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbced330>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "4821e9fb56da06876649d9073a9a2969",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the model’s progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.",
      "metadata": {
        "chunk_id": 58,
        "page_number": 7,
        "page_range": "7",
        "word_count": 73,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbcefe80>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "354e9ce2f6ab6af7385609d598ca5087",
        "category": "CompositeElement"
      }
    },
    {
      "content": "As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\n\n7\n\nFigure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.",
      "metadata": {
        "chunk_id": 59,
        "page_number": 7,
        "page_range": "7",
        "word_count": 42,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbced210>, <unstructured.documents.elements.Footer object at 0x7fb3dbced9c0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db94a350>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "112d25f0ed04856f00a41b62f9b8a875",
        "category": "CompositeElement"
      }
    },
    {
      "content": "ment throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time compu- tation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.",
      "metadata": {
        "chunk_id": 60,
        "page_number": 8,
        "page_range": "8",
        "word_count": 63,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db94bb80>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "37dd336851f446ee597d3d9f78a48734",
        "category": "CompositeElement"
      }
    },
    {
      "content": "One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflection—where the model revisits and reevaluates its previous steps—and the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model’s interaction with the reinforcement learning environment. This spontaneous development",
      "metadata": {
        "chunk_id": 61,
        "page_number": 8,
        "page_range": "8",
        "word_count": 65,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2a89810>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "47c764e25a1d57db3acd624645e2c525",
        "category": "CompositeElement"
      }
    },
    {
      "content": "significantly enhances DeepSeek-R1-Zero’s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.",
      "metadata": {
        "chunk_id": 62,
        "page_number": 8,
        "page_range": "8",
        "word_count": 17,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2a89810>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "b75c17f2b0d1035e1ff75d309619d665",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model’s growing reasoning abilities but also a captivating example of how reinforcement",
      "metadata": {
        "chunk_id": 63,
        "page_number": 8,
        "page_range": "8",
        "word_count": 75,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2a8b0a0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "d928c8054719203724f492e7ef254e4c",
        "category": "CompositeElement"
      }
    },
    {
      "content": "learning can lead to unexpected and sophisticated outcomes.",
      "metadata": {
        "chunk_id": 64,
        "page_number": 8,
        "page_range": "8",
        "word_count": 8,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2a8b0a0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "f5b8186b46ef5c49d0aaa99d7858356e",
        "category": "CompositeElement"
      }
    },
    {
      "content": "This moment is not only an “aha moment” for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The “aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way",
      "metadata": {
        "chunk_id": 65,
        "page_number": 8,
        "page_range": "8",
        "word_count": 80,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2a899f0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "076a01165f64b526d22e1a52df57b34e",
        "category": "CompositeElement"
      }
    },
    {
      "content": "for more autonomous and adaptive models in the future.",
      "metadata": {
        "chunk_id": 66,
        "page_number": 8,
        "page_range": "8",
        "word_count": 9,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2a899f0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "100a7176daa0473dba3b2b87b6d98463",
        "category": "CompositeElement"
      }
    },
    {
      "content": "8\n\nQuestion: If 𝑎 > 1, then the sum of the real solutions of\n\n√︁𝑎 −\n\n√\n\n𝑎 + 𝑥 = 𝑥 is equal to",
      "metadata": {
        "chunk_id": 67,
        "page_number": 8,
        "page_range": "8",
        "word_count": 25,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Footer object at 0x7fb3f2a885b0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db872ef0>, <unstructured.documents.elements.Text object at 0x7fb3db873880>, <unstructured.documents.elements.Text object at 0x7fb3db873bb0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db8723b0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "ae9a3dc9a0b2819481e09ae874250462",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Response: <think> To solve the equation (cid:16)√︁𝑎 − Rearrange to isolate the inner square root term: (𝑎 − 𝑥2)2 = 𝑎 + 𝑥 =⇒ 𝑎2 − 2𝑎𝑥2 + (𝑥2)2 = 𝑎 + 𝑥 =⇒ 𝑥4 − 2𝑎𝑥2 − 𝑥 + (𝑎2 − 𝑎) = 0 . . . Wait, wait. Wait. That’s an aha moment I can flag here. Let’s reevaluate this step-by-step to identify if the correct sum can be · · · We started with the equation: √︁𝑎 − First, let’s square both sides: √ 𝑎 − Next, I could square both sides again, treating the equation: · · · . . .\n\n√\n\n√︁𝑎 − = 𝑥2 =⇒ 𝑎 −",
      "metadata": {
        "chunk_id": 68,
        "page_number": 9,
        "page_range": "9",
        "word_count": 113,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db871db0>, <unstructured.documents.elements.Text object at 0x7fb3db8724d0>, <unstructured.documents.elements.Text object at 0x7fb3db8722c0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "047b75cd8cee6eb0518a2d5680113688",
        "category": "CompositeElement"
      }
    },
    {
      "content": "𝑎 + 𝑥 = 𝑥, let’s start by squaring both · · · √ 𝑎 + 𝑥 = 𝑥2.\n\n√\n\n(cid:17) 2\n\n𝑎 + 𝑥\n\n√\n\n𝑎 + 𝑥 = 𝑥\n\n√\n\n𝑎 + 𝑥 = 𝑥2 =⇒\n\n𝑎 + 𝑥 = 𝑎 − 𝑥2\n\nTable 3 | An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.",
      "metadata": {
        "chunk_id": 69,
        "page_number": 9,
        "page_range": "9",
        "word_count": 86,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db872860>, <unstructured.documents.elements.Text object at 0x7fb3db872470>, <unstructured.documents.elements.Text object at 0x7fb3db8728c0>, <unstructured.documents.elements.Title object at 0x7fb3db873940>, <unstructured.documents.elements.Text object at 0x7fb3db871c00>, <unstructured.documents.elements.Title object at 0x7fb3db8731c0>, <unstructured.documents.elements.Text object at 0x7fb3db871c90>, <unstructured.documents.elements.Text object at 0x7fb3db873250>, <unstructured.documents.elements.Title object at 0x7fb3db870160>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db873a00>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "3487a3b7dc11c09880b02fdfe815fbf0",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data.",
      "metadata": {
        "chunk_id": 70,
        "page_number": 9,
        "page_range": "9",
        "word_count": 58,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db872fb0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "96ad49afa3311dc92dcf6e77d8fa140e",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2.3. DeepSeek-R1: Reinforcement Learning with Cold Start",
      "metadata": {
        "chunk_id": 71,
        "page_number": 9,
        "page_range": "9",
        "word_count": 7,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3db871a80>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "f0b55c15893f6f6ab5159eb9b3e24214",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages,",
      "metadata": {
        "chunk_id": 72,
        "page_number": 9,
        "page_range": "9",
        "word_count": 75,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db873dc0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "f8bf7032ad7a2dc8b10a0050108642fe",
        "category": "CompositeElement"
      }
    },
    {
      "content": "outlined as follows.",
      "metadata": {
        "chunk_id": 73,
        "page_number": 9,
        "page_range": "9",
        "word_count": 3,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db873dc0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "674ed161f58ee23fc4a3dfcb3ae0f08c",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2.3.1. Cold Start",
      "metadata": {
        "chunk_id": 74,
        "page_number": 9,
        "page_range": "9",
        "word_count": 3,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3db873820>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "a3f078b50f63771e70aa260c37a1141b",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and",
      "metadata": {
        "chunk_id": 75,
        "page_number": 9,
        "page_range": "9",
        "word_count": 78,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db873f40>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "2f3ba9201d25448cc162de1e46aa9279",
        "category": "CompositeElement"
      }
    },
    {
      "content": "refining the results through post-processing by human annotators.",
      "metadata": {
        "chunk_id": 76,
        "page_number": 9,
        "page_range": "9",
        "word_count": 8,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db873f40>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "aa60e7a836d38e402e6a4c990d4ad404",
        "category": "CompositeElement"
      }
    },
    {
      "content": "In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\n\n9\n\ninclude:",
      "metadata": {
        "chunk_id": 77,
        "page_number": 9,
        "page_range": "9",
        "word_count": 30,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db873f10>, <unstructured.documents.elements.Footer object at 0x7fb3db873b80>, <unstructured.documents.elements.Title object at 0x7fb3db8029b0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "9cf7a074887d05d8e73f01dae71d6a70",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as |special_token|<reasoning_process>|special_token|<summary>,",
      "metadata": {
        "chunk_id": 78,
        "page_number": 10,
        "page_range": "10",
        "word_count": 69,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3db8027d0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "e5f317c2dc5a1143a5f13a4f06485b28",
        "category": "CompositeElement"
      }
    },
    {
      "content": "where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results.",
      "metadata": {
        "chunk_id": 79,
        "page_number": 10,
        "page_range": "10",
        "word_count": 20,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3db8027d0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "afb7b7ceebc77f2327f59483876944a0",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Potential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models.\n\n2.3.2. Reasoning-oriented Reinforcement Learning",
      "metadata": {
        "chunk_id": 80,
        "page_number": 10,
        "page_range": "10",
        "word_count": 34,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3db801660>, <unstructured.documents.elements.Title object at 0x7fb3db800760>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "2a478e3e5a8eb936ff89b2e31f3f2a29",
        "category": "CompositeElement"
      }
    },
    {
      "content": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts",
      "metadata": {
        "chunk_id": 81,
        "page_number": 10,
        "page_range": "10",
        "word_count": 65,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db8003d0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "84bdf3459d709217c2e5b0e13d0f6884",
        "category": "CompositeElement"
      }
    },
    {
      "content": "involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the model’s performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly",
      "metadata": {
        "chunk_id": 82,
        "page_number": 10,
        "page_range": "10",
        "word_count": 74,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db8003d0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "7af96675380e2a7c1db510de9f7a4702",
        "category": "CompositeElement"
      }
    },
    {
      "content": "summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks.",
      "metadata": {
        "chunk_id": 83,
        "page_number": 10,
        "page_range": "10",
        "word_count": 23,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db8003d0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "82de5928c1e9f0705795892bfa1bfa73",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2.3.3. Rejection Sampling and Supervised Fine-Tuning\n\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model’s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below.",
      "metadata": {
        "chunk_id": 84,
        "page_number": 10,
        "page_range": "10",
        "word_count": 66,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3db801f00>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db802a70>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "130bd059503d76896cd7fba74c70d495",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Reasoning data We curate reasoning prompts and generate reasoning trajectories by perform- ing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output",
      "metadata": {
        "chunk_id": 85,
        "page_number": 10,
        "page_range": "10",
        "word_count": 74,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db800400>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "784a6772321e67b0e654e29b2b47ac29",
        "category": "CompositeElement"
      }
    },
    {
      "content": "is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.",
      "metadata": {
        "chunk_id": 86,
        "page_number": 10,
        "page_range": "10",
        "word_count": 43,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db800400>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "bf6df86bd2dcab91762065a28aed9b2b",
        "category": "CompositeElement"
      }
    },
    {
      "content": "10",
      "metadata": {
        "chunk_id": 87,
        "page_number": 10,
        "page_range": "10",
        "word_count": 1,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Footer object at 0x7fb3db802710>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "0163e2636ddfd8329e8f629eefb1d9dc",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as “hello” we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are",
      "metadata": {
        "chunk_id": 88,
        "page_number": 11,
        "page_range": "11",
        "word_count": 74,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db7d9990>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "bf6fbd81c142751bc02c66631bd34658",
        "category": "CompositeElement"
      }
    },
    {
      "content": "unrelated to reasoning.",
      "metadata": {
        "chunk_id": 89,
        "page_number": 11,
        "page_range": "11",
        "word_count": 3,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db7d9990>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "9729acfb8018e2eba9ca0337d5bcc326",
        "category": "CompositeElement"
      }
    },
    {
      "content": "We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about\n\n800k samples.\n\n2.3.4. Reinforcement Learning for all Scenarios",
      "metadata": {
        "chunk_id": 90,
        "page_number": 11,
        "page_range": "11",
        "word_count": 21,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db7d9900>, <unstructured.documents.elements.Title object at 0x7fb3db7d91b0>, <unstructured.documents.elements.Title object at 0x7fb3db7dac20>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "3f59c60074f52ff1491512172c40fcd3",
        "category": "CompositeElement"
      }
    },
    {
      "content": "To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model’s helpfulness and harmlessness while simultane- ously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical",
      "metadata": {
        "chunk_id": 91,
        "page_number": 11,
        "page_range": "11",
        "word_count": 70,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db7d96f0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "63cb6052b37885825ee306c680674f55",
        "category": "CompositeElement"
      }
    },
    {
      "content": "reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train- ing prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the",
      "metadata": {
        "chunk_id": 92,
        "page_number": 11,
        "page_range": "11",
        "word_count": 74,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db7d96f0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "e24f9b729bc35bcb3c1456fea9762f0d",
        "category": "CompositeElement"
      }
    },
    {
      "content": "entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.",
      "metadata": {
        "chunk_id": 93,
        "page_number": 11,
        "page_range": "11",
        "word_count": 56,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db7d96f0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "91130c1a43d5828580a0c85563d8a263",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2.4. Distillation: Empower Small Models with Reasoning Capability",
      "metadata": {
        "chunk_id": 94,
        "page_number": 11,
        "page_range": "11",
        "word_count": 8,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbc9ca90>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "ead549f2fd09c7db8e1a564a0b3affd0",
        "category": "CompositeElement"
      }
    },
    {
      "content": "To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in §2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5- 14B, Qwen2.5-32B, Llama-3.1-8B, and",
      "metadata": {
        "chunk_id": 95,
        "page_number": 11,
        "page_range": "11",
        "word_count": 65,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9f160>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "220420575347cde9001668ef6e31544b",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1.",
      "metadata": {
        "chunk_id": 96,
        "page_number": 11,
        "page_range": "11",
        "word_count": 15,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9f160>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "d8869d94347f85c33964b515557a04f3",
        "category": "CompositeElement"
      }
    },
    {
      "content": "For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community.\n\n3. Experiment",
      "metadata": {
        "chunk_id": 97,
        "page_number": 11,
        "page_range": "11",
        "word_count": 50,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9d4e0>, <unstructured.documents.elements.ListItem object at 0x7fb3dbc9cd90>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "27926adff03a86fd9540511fd62b55a0",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,\n\n11",
      "metadata": {
        "chunk_id": 98,
        "page_number": 11,
        "page_range": "11",
        "word_count": 59,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9c580>, <unstructured.documents.elements.Footer object at 0x7fb3dbc9ccd0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "effba78e823d782ce4e2f76dcf455a1d",
        "category": "CompositeElement"
      }
    },
    {
      "content": "2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math- ematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage",
      "metadata": {
        "chunk_id": 99,
        "page_number": 12,
        "page_range": "12",
        "word_count": 72,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9d570>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "4235bb9eae543c7aa840cb196982e62f",
        "category": "CompositeElement"
      }
    },
    {
      "content": "GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.",
      "metadata": {
        "chunk_id": 100,
        "page_number": 12,
        "page_range": "12",
        "word_count": 36,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9d570>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "093a35c46a526ed804c33795ae1708ef",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple- evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow",
      "metadata": {
        "chunk_id": 101,
        "page_number": 12,
        "page_range": "12",
        "word_count": 75,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9c100>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "24d67cc38c2f4d28bd6bdc00ad06267a",
        "category": "CompositeElement"
      }
    },
    {
      "content": "their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which",
      "metadata": {
        "chunk_id": 102,
        "page_number": 12,
        "page_range": "12",
        "word_count": 69,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9c100>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "a818b4997198ebe0e3f3f2045b19bac0",
        "category": "CompositeElement"
      }
    },
    {
      "content": "the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark.",
      "metadata": {
        "chunk_id": 103,
        "page_number": 12,
        "page_range": "12",
        "word_count": 43,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9c100>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "fec6b016a1a52dfa7100437563746e7d",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor- mance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a).",
      "metadata": {
        "chunk_id": 104,
        "page_number": 12,
        "page_range": "12",
        "word_count": 47,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9e590>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "a2be4796fac1866c9a6604976ce0881c",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@𝑘 evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-𝑝 value of 0.95 to generate 𝑘 responses (typically between 4 and 64,",
      "metadata": {
        "chunk_id": 105,
        "page_number": 12,
        "page_range": "12",
        "word_count": 76,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9e080>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "1d8770f66d54620aeae3c2efffd63f2e",
        "category": "CompositeElement"
      }
    },
    {
      "content": "depending on the test set size) for each question. Pass@1 is then calculated as",
      "metadata": {
        "chunk_id": 106,
        "page_number": 12,
        "page_range": "12",
        "word_count": 14,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9e080>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "b42f75143c12c90fa051a6b60c9d2ade",
        "category": "CompositeElement"
      }
    },
    {
      "content": "pass@1 =\n\n1 𝑘\n\n𝑘 ∑︁\n\n𝑝𝑖,\n\n𝑖=1\n\nwhere 𝑝𝑖 denotes the correctness of the 𝑖-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64.\n\n1https://aider.chat 2https://codeforces.com 3https://www.cms.org.cn/Home/comp/comp/cid/12.html\n\n12\n\n3.1. DeepSeek-R1 Evaluation\n\nBenchmark (Metric)\n\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek Sonnet-1022\n\n0513",
      "metadata": {
        "chunk_id": 107,
        "page_number": 12,
        "page_range": "12",
        "word_count": 61,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9eaa0>, <unstructured.documents.elements.Title object at 0x7fb3dbc9cd00>, <unstructured.documents.elements.Text object at 0x7fb3dbc9c520>, <unstructured.documents.elements.Text object at 0x7fb3dbc9d000>, <unstructured.documents.elements.Text object at 0x7fb3dbc9f670>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9feb0>, <unstructured.documents.elements.Title object at 0x7fb3dbc9c5e0>, <unstructured.documents.elements.Footer object at 0x7fb3dbc9fe80>, <unstructured.documents.elements.Title object at 0x7fb3dbc9e230>, <unstructured.documents.elements.Title object at 0x7fb3dbc9dd50>, <unstructured.documents.elements.Title object at 0x7fb3f04a5f90>, <unstructured.documents.elements.Text object at 0x7fb3f04a5e40>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "134a445c9788063ae52e4cff59ee4769",
        "category": "CompositeElement"
      }
    },
    {
      "content": "V3\n\no1-mini o1-1217\n\nR1\n\nArchitecture # Activated Params # Total Params\n\n- -\n\n- -\n\nMoE 37B 671B\n\n- -\n\n- -\n\nMoE 37B 671B",
      "metadata": {
        "chunk_id": 108,
        "page_number": 13,
        "page_range": "13",
        "word_count": 25,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3f04a4130>, <unstructured.documents.elements.Text object at 0x7fb3f04a76a0>, <unstructured.documents.elements.Title object at 0x7fb3f04a7850>, <unstructured.documents.elements.Title object at 0x7fb3dbc9dbd0>, <unstructured.documents.elements.ListItem object at 0x7fb3f04a6170>, <unstructured.documents.elements.ListItem object at 0x7fb3dbbcd060>, <unstructured.documents.elements.Title object at 0x7fb3f2aba560>, <unstructured.documents.elements.ListItem object at 0x7fb3f2abaa40>, <unstructured.documents.elements.ListItem object at 0x7fb3f2ab86a0>, <unstructured.documents.elements.Title object at 0x7fb3f2ab91b0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "b78e6d70f7a3618d7fe07218bdbdb3cc",
        "category": "CompositeElement"
      }
    },
    {
      "content": "English\n\nMMLU (Pass@1) MMLU-Redux (EM) MMLU-Pro (EM) DROP (3-shot F1) IF-Eval (Prompt Strict) GPQA Diamond (Pass@1) SimpleQA (Correct) FRAMES (Acc.) AlpacaEval2.0 (LC-winrate) ArenaHard (GPT-4-1106)\n\n88.3 88.9 78.0 88.3 86.5 65.0 28.4 72.5 52.0 85.2\n\n87.2 88.0 72.6 83.7 84.3 49.9 38.2 80.5 51.1 80.4\n\n88.5 89.1 75.9 91.6 86.1 59.1 24.9 73.3 70.0 85.5\n\n85.2 86.7 80.3 83.9 84.8 60.0 7.0 76.9 57.8 92.0\n\n91.8 - - 90.2 - 75.7 47.0 - - -\n\n90.8 92.9 84.0 92.2 83.3 71.5 30.1 82.5 87.6 92.3\n\nCode",
      "metadata": {
        "chunk_id": 109,
        "page_number": 13,
        "page_range": "13",
        "word_count": 85,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbc9cf40>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbc9d150>, <unstructured.documents.elements.Text object at 0x7fb3dbbcc730>, <unstructured.documents.elements.Text object at 0x7fb3dbbcf070>, <unstructured.documents.elements.Text object at 0x7fb3f2ab9f90>, <unstructured.documents.elements.Text object at 0x7fb3f2ab8ee0>, <unstructured.documents.elements.Text object at 0x7fb3f2ab87c0>, <unstructured.documents.elements.Text object at 0x7fb3f2abab90>, <unstructured.documents.elements.Title object at 0x7fb3dbc9f730>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "a5a08292ce479852493cf944ce0911aa",
        "category": "CompositeElement"
      }
    },
    {
      "content": "LiveCodeBench (Pass@1-COT) Codeforces (Percentile) Codeforces (Rating) SWE Verified (Resolved) Aider-Polyglot (Acc.)\n\n38.9 20.3 717 50.8 45.3\n\n32.9 23.6 759 38.8 16.0\n\n36.2 58.7 1134 42.0 49.6\n\n53.8 93.4 1820 41.6 32.9\n\n63.4 96.6 2061 48.9 61.7\n\n65.9 96.3 2029 49.2 53.3\n\nMath\n\nAIME 2024 (Pass@1) MATH-500 (Pass@1) CNMO 2024 (Pass@1)\n\n16.0 78.3 13.1\n\n9.3 74.6 10.8\n\n39.2 90.2 43.2\n\n63.6 90.0 67.6\n\n79.2 96.4 -\n\n79.8 97.3 78.8\n\nChinese",
      "metadata": {
        "chunk_id": 110,
        "page_number": 13,
        "page_range": "13",
        "word_count": 69,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbc9d2a0>, <unstructured.documents.elements.Text object at 0x7fb3dbbccaf0>, <unstructured.documents.elements.Text object at 0x7fb3f406a320>, <unstructured.documents.elements.Text object at 0x7fb3f2ab8b20>, <unstructured.documents.elements.Text object at 0x7fb3f2aba290>, <unstructured.documents.elements.Text object at 0x7fb3f2ab9ea0>, <unstructured.documents.elements.Text object at 0x7fb3f2abada0>, <unstructured.documents.elements.Title object at 0x7fb3dbc9dae0>, <unstructured.documents.elements.Title object at 0x7fb3dbc9fa30>, <unstructured.documents.elements.Text object at 0x7fb3dbbce2c0>, <unstructured.documents.elements.Text object at 0x7fb3f406a110>, <unstructured.documents.elements.Text object at 0x7fb3f2ab89d0>, <unstructured.documents.elements.Text object at 0x7fb3f2abbe50>, <unstructured.documents.elements.Text object at 0x7fb3f2abb520>, <unstructured.documents.elements.Text object at 0x7fb3f2ab8d60>, <unstructured.documents.elements.Title object at 0x7fb3dbc9fa00>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "84cd97fadae9b2c38337e98028924060",
        "category": "CompositeElement"
      }
    },
    {
      "content": "CLUEWSC (EM) C-Eval (EM) C-SimpleQA (Correct)\n\n85.4 76.7 55.4\n\n87.9 76.0 58.7\n\n90.9 86.5 68.0\n\n89.9 68.9 40.3\n\n- -\n\n92.8 91.8 63.7\n\nTable 4 | Comparison between DeepSeek-R1 and other representative models.",
      "metadata": {
        "chunk_id": 111,
        "page_number": 13,
        "page_range": "13",
        "word_count": 33,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbc9f6a0>, <unstructured.documents.elements.Text object at 0x7fb3dbbcf6a0>, <unstructured.documents.elements.Text object at 0x7fb3f406a080>, <unstructured.documents.elements.Text object at 0x7fb3f2abad10>, <unstructured.documents.elements.Text object at 0x7fb3f2ab95a0>, <unstructured.documents.elements.ListItem object at 0x7fb3f2abaaa0>, <unstructured.documents.elements.Text object at 0x7fb3f2abbbb0>, <unstructured.documents.elements.Title object at 0x7fb3f2aba680>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "6304593168644be3a9e963b9c4f534d2",
        "category": "CompositeElement"
      }
    },
    {
      "content": "For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im- provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif- icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of",
      "metadata": {
        "chunk_id": 112,
        "page_number": 13,
        "page_range": "13",
        "word_count": 60,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2abb0d0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "4f1fd971a79fec9ee2004c936fa6914a",
        "category": "CompositeElement"
      }
    },
    {
      "content": "reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an",
      "metadata": {
        "chunk_id": 113,
        "page_number": 13,
        "page_range": "13",
        "word_count": 67,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2abb0d0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "8d9f0724c4cd0e686e58389bedd882b9",
        "category": "CompositeElement"
      }
    },
    {
      "content": "accuracy of over 70%.",
      "metadata": {
        "chunk_id": 114,
        "page_number": 13,
        "page_range": "13",
        "word_count": 4,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2abb0d0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "edfd8eff0a5f2d4ddf53c793933d7a8c",
        "category": "CompositeElement"
      }
    },
    {
      "content": "DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a model’s ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its significant outperformance of",
      "metadata": {
        "chunk_id": 115,
        "page_number": 13,
        "page_range": "13",
        "word_count": 64,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2ab91e0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "ced214b2f7ef82f53945e329cd746891",
        "category": "CompositeElement"
      }
    },
    {
      "content": "DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that",
      "metadata": {
        "chunk_id": 116,
        "page_number": 13,
        "page_range": "13",
        "word_count": 47,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2ab91e0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "9d277b820006633819303144025cdccb",
        "category": "CompositeElement"
      }
    },
    {
      "content": "13\n\nDeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks.",
      "metadata": {
        "chunk_id": 117,
        "page_number": 13,
        "page_range": "13",
        "word_count": 16,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Footer object at 0x7fb3f2ab9330>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db86ead0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "f2987331693be499295a4fa699e5d118",
        "category": "CompositeElement"
      }
    },
    {
      "content": "On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version,",
      "metadata": {
        "chunk_id": 118,
        "page_number": 14,
        "page_range": "14",
        "word_count": 66,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2a8bfa0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "6c553087e3287b2f2a54f373d6afa7f5",
        "category": "CompositeElement"
      }
    },
    {
      "content": "as the amount of related RL training data currently remains very limited.",
      "metadata": {
        "chunk_id": 119,
        "page_number": 14,
        "page_range": "14",
        "word_count": 12,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2a8bfa0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "96564be3715519ed359a2d51336db73c",
        "category": "CompositeElement"
      }
    },
    {
      "content": "3.2. Distilled Model Evaluation\n\nModel\n\nAIME 2024\n\nMATH-500\n\nGPQA Diamond\n\nLiveCode Bench\n\nCodeForces\n\npass@1\n\ncons@64\n\npass@1\n\npass@1\n\npass@1\n\nrating\n\nGPT-4o-0513 Claude-3.5-Sonnet-1022 OpenAI-o1-mini QwQ-32B-Preview\n\n9.3 16.0 63.6 50.0\n\n13.4 26.7 80.0 60.0\n\n74.6 78.3 90.0 90.6\n\n49.9 65.0 60.0 54.5\n\n32.9 38.9 53.8 41.9\n\n759 717 1820 1316",
      "metadata": {
        "chunk_id": 120,
        "page_number": 14,
        "page_range": "14",
        "word_count": 47,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3db86ea70>, <unstructured.documents.elements.Title object at 0x7fb3dbbcac20>, <unstructured.documents.elements.Title object at 0x7fb3dbbc8280>, <unstructured.documents.elements.Title object at 0x7fb3dbbca800>, <unstructured.documents.elements.Title object at 0x7fb3dbde8790>, <unstructured.documents.elements.Title object at 0x7fb3dbde8040>, <unstructured.documents.elements.Title object at 0x7fb3dbdea050>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbdeb7f0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbdeb790>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbde8af0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbdea080>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbde8850>, <unstructured.documents.elements.Title object at 0x7fb3dbde8820>, <unstructured.documents.elements.Title object at 0x7fb3dbde8c40>, <unstructured.documents.elements.Text object at 0x7fb3dbdeb2e0>, <unstructured.documents.elements.Text object at 0x7fb3dbdeb8e0>, <unstructured.documents.elements.Text object at 0x7fb3dbdea4a0>, <unstructured.documents.elements.Text object at 0x7fb3dbde9000>, <unstructured.documents.elements.Text object at 0x7fb3dbdebd90>, <unstructured.documents.elements.Text object at 0x7fb3dbde84f0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "656494332397a69169b45edd0d964fb9",
        "category": "CompositeElement"
      }
    },
    {
      "content": "DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-14B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Llama-70B\n\n28.9 55.5 69.7 72.6 50.4 70.0\n\n52.7 83.3 80.0 83.3 80.0 86.7\n\n83.9 92.8 93.9 94.3 89.1 94.5\n\n33.8 49.1 59.1 62.1 49.0 65.2\n\n16.9 37.6 53.1 57.2 39.6 57.5\n\n954 1189 1481 1691 1205 1633\n\nTable 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.",
      "metadata": {
        "chunk_id": 121,
        "page_number": 14,
        "page_range": "14",
        "word_count": 57,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbde9360>, <unstructured.documents.elements.Text object at 0x7fb3dbdebd30>, <unstructured.documents.elements.Text object at 0x7fb3dbdeae00>, <unstructured.documents.elements.Text object at 0x7fb3dbde9b70>, <unstructured.documents.elements.Text object at 0x7fb3dbdea740>, <unstructured.documents.elements.Text object at 0x7fb3dbde9060>, <unstructured.documents.elements.Text object at 0x7fb3dbdeabf0>, <unstructured.documents.elements.Text object at 0x7fb3dbdea440>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "b37dae069281371cea12dd21c7465729",
        "category": "CompositeElement"
      }
    },
    {
      "content": "As shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek- R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non- reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B- Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla- tion. Additionally, we found that applying",
      "metadata": {
        "chunk_id": 122,
        "page_number": 14,
        "page_range": "14",
        "word_count": 61,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbde83d0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "3fc84785cfd0edd74fa0dfcd4201486b",
        "category": "CompositeElement"
      }
    },
    {
      "content": "RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.",
      "metadata": {
        "chunk_id": 123,
        "page_number": 14,
        "page_range": "14",
        "word_count": 27,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbde83d0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "5226904e48eccba6c5e72d1308ea4110",
        "category": "CompositeElement"
      }
    },
    {
      "content": "4. Discussion\n\n4.1. Distillation v.s. Reinforcement Learning\n\nIn Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?",
      "metadata": {
        "chunk_id": 124,
        "page_number": 14,
        "page_range": "14",
        "word_count": 48,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3dbdeaa10>, <unstructured.documents.elements.Title object at 0x7fb3dbde9720>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbde8310>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "d62b9f8b61066c9e311bace721f0b906",
        "category": "CompositeElement"
      }
    },
    {
      "content": "To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\n\n14\n\nAIME 2024\n\nMATH-500 GPQA Diamond LiveCodeBench\n\nModel\n\npass@1\n\ncons@64\n\npass@1\n\npass@1\n\npass@1",
      "metadata": {
        "chunk_id": 125,
        "page_number": 14,
        "page_range": "14",
        "word_count": 53,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbdeb3a0>, <unstructured.documents.elements.Footer object at 0x7fb3dbde8e50>, <unstructured.documents.elements.Title object at 0x7fb3db8e1db0>, <unstructured.documents.elements.Title object at 0x7fb3db8e1f30>, <unstructured.documents.elements.Title object at 0x7fb3db8e1d50>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db8e1e10>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db8e1ea0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db8e0dc0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db8e2230>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db8e23e0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "012c09f3d1c74c643d029d4247c81c43",
        "category": "CompositeElement"
      }
    },
    {
      "content": "QwQ-32B-Preview DeepSeek-R1-Zero-Qwen-32B DeepSeek-R1-Distill-Qwen-32B\n\n50.0 47.0 72.6\n\n60.0 60.0 83.3\n\n90.6 91.6 94.3\n\n54.5 55.0 62.1\n\n41.9 40.2 57.2\n\nTable 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\n\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1- Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks.",
      "metadata": {
        "chunk_id": 126,
        "page_number": 15,
        "page_range": "15",
        "word_count": 54,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3db8e2140>, <unstructured.documents.elements.Text object at 0x7fb3db8e2ce0>, <unstructured.documents.elements.Text object at 0x7fb3db8e2500>, <unstructured.documents.elements.Text object at 0x7fb3db8005e0>, <unstructured.documents.elements.Text object at 0x7fb3db802b60>, <unstructured.documents.elements.Text object at 0x7fb3db801240>, <unstructured.documents.elements.Title object at 0x7fb3db8028c0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db8034f0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "5146a1ccdbb60a5d51d4c9425c57763a",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger- scale reinforcement",
      "metadata": {
        "chunk_id": 127,
        "page_number": 15,
        "page_range": "15",
        "word_count": 68,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db803670>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "d5daa7ab48d18bba045d54dde0055f1e",
        "category": "CompositeElement"
      }
    },
    {
      "content": "learning.",
      "metadata": {
        "chunk_id": 128,
        "page_number": 15,
        "page_range": "15",
        "word_count": 1,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db803670>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "2d5fab1ac2b4046639cfb256fa23ead5",
        "category": "CompositeElement"
      }
    },
    {
      "content": "4.2. Unsuccessful Attempts\n\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.",
      "metadata": {
        "chunk_id": 129,
        "page_number": 15,
        "page_range": "15",
        "word_count": 43,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbcec250>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbcee050>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "34b9f03d05aa96eb028f7c987e66192f",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc- cess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may",
      "metadata": {
        "chunk_id": 130,
        "page_number": 15,
        "page_range": "15",
        "word_count": 78,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbcecc70>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "ffe3daaa0ac5422353bcd77c466fa837",
        "category": "CompositeElement"
      }
    },
    {
      "content": "not yield satisfactory results, while manual annotation is not con- ducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared",
      "metadata": {
        "chunk_id": 131,
        "page_number": 15,
        "page_range": "15",
        "word_count": 78,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbcecc70>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "2ab8b0fd15fd3917618edd30f33f19b9",
        "category": "CompositeElement"
      }
    },
    {
      "content": "to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.",
      "metadata": {
        "chunk_id": 132,
        "page_number": 15,
        "page_range": "15",
        "word_count": 16,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbcecc70>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "b23278fb9347893d5aa8389275d05a16",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil- ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use",
      "metadata": {
        "chunk_id": 133,
        "page_number": 15,
        "page_range": "15",
        "word_count": 76,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbcef160>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "6438b5fedeacbc649ef8129b24e0783a",
        "category": "CompositeElement"
      }
    },
    {
      "content": "collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process.",
      "metadata": {
        "chunk_id": 134,
        "page_number": 15,
        "page_range": "15",
        "word_count": 34,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbcef160>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "7b77b34991c14fb56a756bf18ea70608",
        "category": "CompositeElement"
      }
    },
    {
      "content": "However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an\n\n15",
      "metadata": {
        "chunk_id": 135,
        "page_number": 15,
        "page_range": "15",
        "word_count": 26,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbcec040>, <unstructured.documents.elements.Footer object at 0x7fb3dbcec9d0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "f689b82e3da3a38f09fa994291d653c1",
        "category": "CompositeElement"
      }
    },
    {
      "content": "exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process. Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve. While AlphaGo’s core success relied on training a value model to progressively enhance its",
      "metadata": {
        "chunk_id": 136,
        "page_number": 16,
        "page_range": "16",
        "word_count": 79,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db8e1960>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "65dae2f0c8c2b578c237c5ecbe92ca6f",
        "category": "CompositeElement"
      }
    },
    {
      "content": "performance, this principle proves difficult to replicate in our setup due to the complexities of token generation.",
      "metadata": {
        "chunk_id": 137,
        "page_number": 16,
        "page_range": "16",
        "word_count": 17,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db8e1960>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "ef8bf537ca6694611142a953244b7584",
        "category": "CompositeElement"
      }
    },
    {
      "content": "In conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge.\n\n5. Conclusion, Limitations, and Future Work",
      "metadata": {
        "chunk_id": 138,
        "page_number": 16,
        "page_range": "16",
        "word_count": 32,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db8e2350>, <unstructured.documents.elements.ListItem object at 0x7fb3db8e0910>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "a719f8d406e93135add1a8483ba7884d",
        "category": "CompositeElement"
      }
    },
    {
      "content": "In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on a range of tasks.",
      "metadata": {
        "chunk_id": 139,
        "page_number": 16,
        "page_range": "16",
        "word_count": 55,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db8e3a60>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "8ce4da8c13c1b5c5996ec8aac4472e2e",
        "category": "CompositeElement"
      }
    },
    {
      "content": "We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction- tuned models based on the same underlying",
      "metadata": {
        "chunk_id": 140,
        "page_number": 16,
        "page_range": "16",
        "word_count": 67,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db8e1cc0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "6bc1372294d8d33c2efccd8aad4deb5c",
        "category": "CompositeElement"
      }
    },
    {
      "content": "checkpoints.",
      "metadata": {
        "chunk_id": 141,
        "page_number": 16,
        "page_range": "16",
        "word_count": 1,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db8e1cc0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "245c7b8ab524ca0b7c0c766798f6dcfe",
        "category": "CompositeElement"
      }
    },
    {
      "content": "In the future, we plan to invest in research across the following directions for DeepSeek-R1.\n\nGeneral Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.",
      "metadata": {
        "chunk_id": 142,
        "page_number": 16,
        "page_range": "16",
        "word_count": 56,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db8e1bd0>, <unstructured.documents.elements.ListItem object at 0x7fb3db8e1720>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "b3340174ec90b7732ce6a11a51e64a69",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.",
      "metadata": {
        "chunk_id": 143,
        "page_number": 16,
        "page_range": "16",
        "word_count": 55,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3db8e12a0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "ab05a83020abd5fe5555b75765dcdeb8",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.",
      "metadata": {
        "chunk_id": 144,
        "page_number": 16,
        "page_range": "16",
        "word_count": 39,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3db8e19c0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "7c12cba67faa5aa2682265dd4d664070",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Software Engineering Tasks: Due to the long evaluation times, which impact the effi- ciency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency.\n\n16",
      "metadata": {
        "chunk_id": 145,
        "page_number": 16,
        "page_range": "16",
        "word_count": 70,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.ListItem object at 0x7fb3db8e1900>, <unstructured.documents.elements.Footer object at 0x7fb3db8e3c10>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "e393bbcfdfef572672d1228b188514e9",
        "category": "CompositeElement"
      }
    },
    {
      "content": "References\n\nAI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m\n\nodels/blob/main/models/llama3_1/MODEL_CARD.md.\n\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3\n\n5-sonnet.",
      "metadata": {
        "chunk_id": 146,
        "page_number": 17,
        "page_range": "17",
        "word_count": 18,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3db7d8a00>, <unstructured.documents.elements.Title object at 0x7fb3dbc126e0>, <unstructured.documents.elements.Title object at 0x7fb3dbc11a50>, <unstructured.documents.elements.Title object at 0x7fb3dbc11270>, <unstructured.documents.elements.ListItem object at 0x7fb3dbc135e0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "a3af72c9837bbb6638e6beff5500b65b",
        "category": "CompositeElement"
      }
    },
    {
      "content": "M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike,",
      "metadata": {
        "chunk_id": 147,
        "page_number": 17,
        "page_range": "17",
        "word_count": 94,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f40260e0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "933da6e12130c58b3f433f8f29881438",
        "category": "CompositeElement"
      }
    },
    {
      "content": "J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.",
      "metadata": {
        "chunk_id": 148,
        "page_number": 17,
        "page_range": "17",
        "word_count": 41,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f40260e0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "5b8ca3425607f0b0623e260e1d8b7f98",
        "category": "CompositeElement"
      }
    },
    {
      "content": "A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple\n\nway to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.",
      "metadata": {
        "chunk_id": 149,
        "page_number": 17,
        "page_range": "17",
        "word_count": 55,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f40265f0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f4025ba0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f4026560>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "5ef3141ffefb25391f3689323ae04658",
        "category": "CompositeElement"
      }
    },
    {
      "content": "X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https: //arxiv.org/abs/2309.17179.\n\nL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL\n\nhttps://arxiv.org/abs/2210.10760.",
      "metadata": {
        "chunk_id": 150,
        "page_number": 17,
        "page_range": "17",
        "word_count": 46,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f4026020>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f4025f00>, <unstructured.documents.elements.Text object at 0x7fb3f4026a10>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "55ebc2b59bd6de6ea2e78a4320fe6dc0",
        "category": "CompositeElement"
      }
    },
    {
      "content": "A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127.\n\nGoogle. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno",
      "metadata": {
        "chunk_id": 151,
        "page_number": 17,
        "page_range": "17",
        "word_count": 61,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2afdd20>, <unstructured.documents.elements.Title object at 0x7fb3f2afe590>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "1336a6774f8eb681811cebfd1eb0f571",
        "category": "CompositeElement"
      }
    },
    {
      "content": "logy/ai/google-gemini-next-generation-model-february-2024.\n\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi- nese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024.\n\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\n\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.",
      "metadata": {
        "chunk_id": 152,
        "page_number": 17,
        "page_range": "17",
        "word_count": 62,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3f2afe5f0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db802b90>, <unstructured.documents.elements.Text object at 0x7fb3f2afc3a0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f2afc940>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "25e435daf7af702ae16fd311474e3720",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\n\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.\n\n17",
      "metadata": {
        "chunk_id": 153,
        "page_number": 17,
        "page_range": "17",
        "word_count": 75,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2afdfc0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f2afd7b0>, <unstructured.documents.elements.Footer object at 0x7fb3f2afec80>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "5013fe87c450972fdc5f4a6986c78050",
        "category": "CompositeElement"
      }
    },
    {
      "content": "S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR, abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941.",
      "metadata": {
        "chunk_id": 154,
        "page_number": 18,
        "page_range": "18",
        "word_count": 33,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2afcee0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "9213fb09dda2d6f3853b6382fda7cefa",
        "category": "CompositeElement"
      }
    },
    {
      "content": "A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\n\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur- ing massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023.",
      "metadata": {
        "chunk_id": 155,
        "page_number": 18,
        "page_range": "18",
        "word_count": 65,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2afe830>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f2afd990>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "0e967e715044eb57e9bc68eda693144b",
        "category": "CompositeElement"
      }
    },
    {
      "content": "T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024.\n\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\n\nB. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL",
      "metadata": {
        "chunk_id": 156,
        "page_number": 18,
        "page_range": "18",
        "word_count": 76,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2affe50>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f2afd9f0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f2afd240>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "08318319d00a2547c54b0652379bc091",
        "category": "CompositeElement"
      }
    },
    {
      "content": "https://github.com/WildEval/ZeroEval.\n\nMAA. American invitational mathematics examination - aime.\n\nIn American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math -competitions/american-invitational-mathematics-examination-aime.\n\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\n\nOpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin",
      "metadata": {
        "chunk_id": 157,
        "page_number": 18,
        "page_range": "18",
        "word_count": 36,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3f2afe7d0>, <unstructured.documents.elements.Title object at 0x7fb3f2afeb60>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f2aff790>, <unstructured.documents.elements.Title object at 0x7fb3f2aff040>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f2aff160>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "3d5ba1e8108c9ca0965baafcce50692a",
        "category": "CompositeElement"
      }
    },
    {
      "content": "g-to-reason-with-llms/.\n\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing\n\nsimpleqa/.\n\nOpenAI. Introducing SWE-bench verified we’re releasing a human-validated subset of swe- bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench -verified/.\n\nQwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm\n\n.github.io/blog/qwq-32b-preview/.\n\nQwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b",
      "metadata": {
        "chunk_id": 158,
        "page_number": 18,
        "page_range": "18",
        "word_count": 50,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3f2affeb0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f2afd120>, <unstructured.documents.elements.ListItem object at 0x7fb3f2afca60>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f2affbe0>, <unstructured.documents.elements.Text object at 0x7fb3f2afd060>, <unstructured.documents.elements.Title object at 0x7fb3f2aff760>, <unstructured.documents.elements.Title object at 0x7fb3f2afd600>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "14f88874472bccb038b75723e0b88b81",
        "category": "CompositeElement"
      }
    },
    {
      "content": "log/qwen2.5.\n\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\n\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.",
      "metadata": {
        "chunk_id": 159,
        "page_number": 18,
        "page_range": "18",
        "word_count": 66,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3f2afdb70>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f2afe650>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f2afe410>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "cead0666a404009f4ed770b6d5f2eaa9",
        "category": "CompositeElement"
      }
    },
    {
      "content": "D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017a. URL http://arxiv.org/abs/1712.01815.\n\n18",
      "metadata": {
        "chunk_id": 160,
        "page_number": 18,
        "page_range": "18",
        "word_count": 46,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f2afe8f0>, <unstructured.documents.elements.Footer object at 0x7fb3f2afddb0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "d7232c86f4b69aab1a2185c1079280c6",
        "category": "CompositeElement"
      }
    },
    {
      "content": "D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354–359, 2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270.",
      "metadata": {
        "chunk_id": 161,
        "page_number": 19,
        "page_range": "19",
        "word_count": 53,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbdebf10>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "7f76275553679aeb837120fa94696f86",
        "category": "CompositeElement"
      }
    },
    {
      "content": "C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033 14.\n\nT. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human\n\ndemonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5.",
      "metadata": {
        "chunk_id": 162,
        "page_number": 19,
        "page_range": "19",
        "word_count": 47,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbdea620>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbde8430>, <unstructured.documents.elements.Text object at 0x7fb3dbde8520>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "4fb09423e30d641df5e94c0e309f3b20",
        "category": "CompositeElement"
      }
    },
    {
      "content": "J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\n\nP. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label- free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023.",
      "metadata": {
        "chunk_id": 163,
        "page_number": 19,
        "page_range": "19",
        "word_count": 65,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbde8700>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbde8d90>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "087462db0e31fd17ac3aa88cfc271f92",
        "category": "CompositeElement"
      }
    },
    {
      "content": "X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.",
      "metadata": {
        "chunk_id": 164,
        "page_number": 19,
        "page_range": "19",
        "word_count": 30,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbde95a0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "e7431ebe92c730f30dbb63195b37129b",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024. URL https://doi.org/10.48550/arXiv.2406.01574.\n\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software",
      "metadata": {
        "chunk_id": 165,
        "page_number": 19,
        "page_range": "19",
        "word_count": 64,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbdeaaa0>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbde9570>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "b589a12c8abb21672d4398f777841874",
        "category": "CompositeElement"
      }
    },
    {
      "content": "engineering agents. arXiv preprint, 2024.\n\nH. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152.\n\nJ. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following",
      "metadata": {
        "chunk_id": 166,
        "page_number": 19,
        "page_range": "19",
        "word_count": 75,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbde8d30>, <unstructured.documents.elements.NarrativeText object at 0x7fb3dbdea7d0>, <unstructured.documents.elements.Text object at 0x7fb3dbde9e10>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "54ac28853b1b32ce32b6403f86d11c93",
        "category": "CompositeElement"
      }
    },
    {
      "content": "evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.\n\n19\n\nAppendix\n\nA. Contributions and Acknowledgments\n\nCore Contributors Daya Guo Dejian Yang Haowei Zhang Junxiao Song Ruoyu Zhang Runxin Xu Qihao Zhu Shirong Ma Peiyi Wang Xiao Bi Xiaokang Zhang Xingkai Yu Yu Wu Z.F. Wu Zhibin Gou Zhihong Shao Zhuoshu Li Ziyi Gao",
      "metadata": {
        "chunk_id": 167,
        "page_number": 19,
        "page_range": "19",
        "word_count": 53,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3dbdeba60>, <unstructured.documents.elements.Footer object at 0x7fb3dbdea4d0>, <unstructured.documents.elements.Title object at 0x7fb3dbde8d60>, <unstructured.documents.elements.Title object at 0x7fb3dbdeaf50>, <unstructured.documents.elements.NarrativeText object at 0x7fb3db9e2c80>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "d0c97b57a596c98e532f8512f2a38097",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Contributors Aixin Liu Bing Xue Bingxuan Wang Bochao Wu Bei Feng Chengda Lu Chenggang Zhao Chengqi Deng Chong Ruan Damai Dai Deli Chen Dongjie Ji Erhang Li Fangyun Lin Fucong Dai Fuli Luo* Guangbo Hao Guanting Chen Guowei Li H. Zhang Hanwei Xu Honghui Ding Huazuo Gao Hui Qu\n\n20",
      "metadata": {
        "chunk_id": 168,
        "page_number": 20,
        "page_range": "20",
        "word_count": 50,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Text object at 0x7fb3db9e3580>, <unstructured.documents.elements.Footer object at 0x7fb3db9e0280>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "62b8aec6c40d0157d0c9541f77dfa949",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Hui Li Jianzhong Guo Jiashi Li Jingchang Chen Jingyang Yuan Jinhao Tu Junjie Qiu Junlong Li J.L. Cai Jiaqi Ni Jian Liang Jin Chen Kai Dong Kai Hu* Kaichao You Kaige Gao Kang Guan Kexin Huang Kuai Yu Lean Wang Lecong Zhang Liang Zhao Litong Wang Liyue Zhang Lei Xu Leyi Xia Mingchuan Zhang Minghua Zhang Minghui Tang Mingxu Zhou Meng Li Miaojun Wang Mingming Li Ning Tian Panpan Huang Peng Zhang Qiancheng Wang Qinyu Chen Qiushi Du Ruiqi Ge* Ruisong Zhang Ruizhe Pan Runji Wang R.J. Chen R.L. Jin",
      "metadata": {
        "chunk_id": 169,
        "page_number": 20,
        "page_range": "20",
        "word_count": 90,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3db9e1f30>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "4ecfe8dd9b63915e5378d27c88493745",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Ruyi Chen Shanghao Lu Shangyan Zhou Shanhuang Chen Shengfeng Ye Shiyu Wang Shuiping Yu Shunfeng Zhou Shuting Pan S.S. Li Shuang Zhou Shaoqing Wu Shengfeng Ye Tao Yun Tian Pei Tianyu Sun T. Wang Wangding Zeng Wen Liu Wenfeng Liang Wenjun Gao Wenqin Yu* Wentao Zhang W.L. Xiao Wei An Xiaodong Liu Xiaohan Wang Xiaokang Chen Xiaotao Nie Xin Cheng Xin Liu Xin Xie Xingchao Liu Xinyu Yang Xinyuan Li Xuecheng Su Xuheng Lin X.Q. Li Xiangyue Jin Xiaojin Shen Xiaosha Chen Xiaowen Sun Xiaoxiang Wang Xinnan",
      "metadata": {
        "chunk_id": 170,
        "page_number": 21,
        "page_range": "21",
        "word_count": 87,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f041e5f0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "e040227ff200207974c147d62f1c9d5e",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Song Xinyi Zhou Xianzu Wang Xinxia Shan Y.K. Li Y.Q. Wang",
      "metadata": {
        "chunk_id": 171,
        "page_number": 21,
        "page_range": "21",
        "word_count": 11,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f041e5f0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "a251d0d7ee54aa7eba89188e630044c3",
        "category": "CompositeElement"
      }
    },
    {
      "content": "21",
      "metadata": {
        "chunk_id": 172,
        "page_number": 21,
        "page_range": "21",
        "word_count": 1,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Footer object at 0x7fb3f041c160>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "d6e5260ea8d92283f9d33035579350aa",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Y.X. Wei Yang Zhang Yanhong Xu Yao Li Yao Zhao Yaofeng Sun Yaohui Wang Yi Yu Yichao Zhang Yifan Shi Yiliang Xiong Ying He Yishi Piao Yisong Wang Yixuan Tan Yiyang Ma* Yiyuan Liu Yongqiang Guo Yuan Ou Yuduan Wang Yue Gong Yuheng Zou Yujia He Yunfan Xiong Yuxiang Luo Yuxiang You Yuxuan Liu Yuyang Zhou Y.X. Zhu Yanping Huang Yaohui Li Yi Zheng Yuchen Zhu Yunxian Ma Ying Tang Yukun Zha Yuting Yan Z.Z. Ren Zehui Ren Zhangli Sha Zhe Fu Zhean Xu Zhenda Xie Zhengyan Zhang Zhewen Hao Zhicheng Ma Zhigang",
      "metadata": {
        "chunk_id": 173,
        "page_number": 21,
        "page_range": "21",
        "word_count": 93,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f041c5b0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "65170d9986aae8248674ed891ef041cf",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Yan Zhiyu Wu Zihui Gu",
      "metadata": {
        "chunk_id": 174,
        "page_number": 21,
        "page_range": "21",
        "word_count": 5,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.NarrativeText object at 0x7fb3f041c5b0>]",
        "is_continuation": true,
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "25dd8ec4cd86755a264844f427710c12",
        "category": "CompositeElement"
      }
    },
    {
      "content": "Zijia Zhu Zijun Liu* Zilin Li Ziwei Xie Ziyang Song Zizheng Pan\n\nZhen Huang Zhipeng Xu Zhongyu Zhang Zhen Zhang\n\nWithin each role, authors are listed alphabetically by the first name. Names marked with *\n\ndenote individuals who have departed from our team.\n\n22",
      "metadata": {
        "chunk_id": 175,
        "page_number": 22,
        "page_range": "22",
        "word_count": 44,
        "file_directory": "temp",
        "filename": "DeepSeek_R1.pdf",
        "languages": [
          "eng"
        ],
        "last_modified": "2025-02-14T18:52:51",
        "orig_elements": "[<unstructured.documents.elements.Title object at 0x7fb3dbc9c5b0>, <unstructured.documents.elements.Title object at 0x7fb3dbc9c820>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f04a6020>, <unstructured.documents.elements.NarrativeText object at 0x7fb3f04a6ef0>, <unstructured.documents.elements.Footer object at 0x7fb3f04a5ea0>]",
        "filetype": "application/pdf",
        "element_type": "CompositeElement",
        "id": "f77bce4b58c73ef55a90426db4c09de8",
        "category": "CompositeElement"
      }
    }
  ]
}