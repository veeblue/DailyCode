{
  "filename": "DeepSeek-R1-技术报告中文版.pdf",
  "total_chunks": 5,
  "total_pages": 5,
  "loading_method": "pypdf",
  "loading_strategy": null,
  "chunking_strategy": null,
  "chunking_method": "loaded",
  "timestamp": "2025-03-12T22:58:30.376081",
  "chunks": [
    {
      "content": "AIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100\nAccuracy / Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3DeepSeek-R1 ：通过强化学习激励 LLMs 中的推理能力\n深度探索人 工 智 能\nresearch@deepseek.com\n摘要\n我们推出了第一代推理模型， DeepSeek-R1-Zero 和 DeepSeek-R1 。DeepSeek-R1-Zero 是一个通\n过大规模强化学习（ RL ）训练而成的模型，无需监督微调（ SFT ）作为初步步骤，展示了卓越\n的推理能力。通过  RL ，DeepSeek-R1-Zero 自然涌现出许多强大且有趣的推理行为。然而，它也\n面临诸如可读性差和语言混合等挑战。为了解决这些问题并进一步提升推理性能，我们推出了  \nDeepSeek-R1 ，它在 RL 之前引入了多阶段训练和冷启动数据。 DeepSeek-R1 在推理任务上实现\n了与 OpenAI-o1-1217 相当的性能。为了支持研究社区，我们开源了  DeepSeek-R1-Zero 、DeepSe\nek-R1 以及基于  Qwen 和 Llama 从 DeepSeek-R1 蒸馏出的六个密集模型（ 1.5B 、7B 、8B 、14B 、\n32B 、70B ）。\n图1 | DeepSeek-R1 的基准性能。",
      "metadata": {
        "chunk_id": 1,
        "page_number": 1,
        "page_range": "1",
        "word_count": 90
      }
    },
    {
      "content": "目录\n1 引言 3\n1.1 贡献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4  \u00001.2 评估结果总结  . . . . . . . . . . . . . . . . \n. . . . . . . . . . . . . 4\n2 方法 5\n2.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5  \u00002.2 DeepSeek-R1-Zero ：基础模型\n上的强化学习  . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 强化学习算法  . . . . . . . . . . . . . . . . . . . . . . 5 2.2.2 奖励建模  . . . . . . . . . . . . . . . . . . . . . . \n. . . . . . . . . . 6 2.2.3 训练模板  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2.4 DeepSeek-R1-Z\nero 的性能、自我进化过程和顿悟时刻  6\n2.3 DeepSeek-R1 ：冷启动强化学习  . . . . . . . . . . . . . . . 9  \u00002.3.1 冷启动 . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . 9  \u00002.3.2 面向推理的强化学习  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10  \u00002.3.3 拒绝\n采样与监督微调  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10  \u00002.3.4 全场景强化学习  . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . 11  \u00002.4 蒸馏：赋予小模型推理能力  . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3 实验 11\n3.1 DeepSeek-R1 评估 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 蒸馏模型评估  . . . . . . . . . . \n. . . . . . . . . . . . . . . . . . . . . 14\n4 讨论 14\n4.1 蒸馏与强化学习  . . . . . . . . . . . . . . . . . . . . . . . . 14 4.2 未成功的尝试  . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . 15\n5 结论、局限性与未来工作  16\nA 贡献与致谢  20\n2",
      "metadata": {
        "chunk_id": 2,
        "page_number": 2,
        "page_range": "2",
        "word_count": 588
      }
    },
    {
      "content": "1. 引言\n近年来，大型语言模型（ LLMs ）经历了快速的迭代和进化（ Anthropic, 2024; Google, 2024; Ope\nnAI, 2024a ），逐步缩小了与人工通用智能（ AGI ）之间的差距。\n最近，后训练已成为完整训练流程中的一个重要组成部分。它已被证明可以提高推理任务的\n准确性，与社会价值观保持一致，并适应用户偏好，同时相对于预训练所需的计算资源相对较\n少。在推理能力方面， OpenAI 的o1 （OpenAI, 2024b ）系列模型首次通过增加思维链推理过程的\n长度引入了推理时扩展。这种方法在数学、编码和科学推理等各种推理任务中取得了显著改进\n。然而，有效的测试时扩展仍然是研究界的一个开放性问题。之前的一些工作探索了各种方法\n，包括基于过程的奖励模型（ Lightman 等，2023 ；Uesato 等，2022 ；Wang 等，2023 ）、强化学\n习（Kumar 等，2024 ）以及蒙特卡洛树搜索和束搜索等搜索算法（ Feng 等，2024 ；Trinh 等，202\n4 ；Xin 等，2024 ）。然而，这些方法都没有达到与 OpenAI 的o1 系列模型相媲美的通用推理性能\n。\n在本文中，我们迈出了利用纯强化学习（ RL ）提升语言模型推理能力的第一步。我们的目标\n是探索大型语言模型（ LLMs ）在没有监督数据的情况下发展推理能力的潜力，重点关注它们通\n过纯RL 过程的自我进化。具体而言，我们使用 DeepSeek-V3-Base 作为基础模型，并采用 GRPO （\nShao 等，2024 ）作为RL 框架，以提升模型在推理任务中的表现。在训练过程中， DeepSeek-R1-Z\nero 自然涌现出许多强大且有趣的推理行为。经过数千次 RL 步骤后， DeepSeek-R1-Zero 在推理基\n准测试中展现出卓越的性能。例如， AIME 2024 上的pass@1 得分从15.6% 提升至71.0% ，而在多\n数投票机制下，得分进一步提高至 86.7% ，与OpenAI-o1-0912 的性能相当。\n然而，DeepSeek-R1-Zero 遇到了诸如可读性差和语言混合等挑战。为了解决这些问题并进一\n步提升推理性能，我们引入了  DeepSeek-R1 ，它结合了少量冷启动数据和多阶段训练流程。具体\n来说，我们首先收集数千条冷启动数据来微调  DeepSeek-V3-Base 模型。随后，我们进行类似  De\nepSeek-R1-Zero 的推理导向强化学习（ RL ）。在 RL 过程接近收敛时，我们通过对  RL 检查点进\n行拒绝采样，结合来自  DeepSeek-V3 在写作、事实问答和自我认知等领域的监督数据，创建新\n的 SFT 数据，然后重新训练  DeepSeek-V3-Base 模型。使用新数据微调后，检查点会经历额外的  \nRL 过程，考虑所有场景的提示。经过这些步骤，我们获得了称为  DeepSeek-R1 的检查点，其性\n能与 OpenAI-o1-1217 相当。\n我们进一步探索了从 DeepSeek-R1 到更小密集模型的蒸馏过程。以 Qwen2.5-32B （Qwen, 2024\nb ）为基础模型，直接从 DeepSeek-R1 进行蒸馏的效果优于在其上应用强化学习。这表明，更大\n基础模型发现的推理模式对于提升推理能力至关重要。我们开源了蒸馏后的 Qwen 和Llama （Dub\ney 等，2024 ）系列。值得注意的是，我们蒸馏的 14B 模型大幅超越了当前最先进的开源 QwQ-32B\n-Preview （Qwen, 2024a ），而蒸馏的 32B 和70B 模型在密集模型的推理基准测试中创下了新纪录\n。\n3",
      "metadata": {
        "chunk_id": 3,
        "page_number": 3,
        "page_range": "3",
        "word_count": 143
      }
    },
    {
      "content": "1.1. 贡献\n训练后：在基础模型上进行大规模强化学习\n• 我们直接将强化学习（ RL ）应用于基础模型，而不依赖于监督微调（ SFT ）作为初步步骤\n。这种方法使模型能够探索思维链（ CoT ）以解决复杂问题，从而开发出 DeepSeek-R1-Zer\no 。DeepSeek-R1-Zero 展示了自我验证、反思和生成长思维链等能力，标志着研究社区的一\n个重要里程碑。值得注意的是，这是首个公开的研究，验证了 LLMs 的推理能力可以纯粹\n通过RL 激励，而不需要 SFT 。这一突破为这一领域的未来进展铺平了道路。\n• 我们介绍了开发 DeepSeek-R1 的流程。该流程包含两个 RL 阶段，旨在发现改进的推理模式\n并与人类偏好对齐，以及两个 SFT 阶段，作为模型推理和非推理能力的基础。我们相信该\n流程将通过创建更好的模型为行业带来益处。\n蒸馏：小型模型也能强大\n• 我们证明了较大模型的推理模式可以被提炼到较小的模型中，与通过强化学习在小模型上\n发现的推理模式相比，性能更优。开源的 DeepSeek-R1 及其API 将有益于研究社区在未来提\n炼出更好的小型模型。\n• 利用DeepSeek-R1 生成的推理数据，我们对研究社区中广泛使用的多个密集模型进行了微\n调。评估结果表明，经过蒸馏的较小密集模型在基准测试中表现尤为出色。 DeepSeek-R1-\nDistill-Qwen-7B 在AIME 2024 上达到了 55.5% ，超越了 QwQ-32B-Preview 。此外， DeepSeek-\nR1-Distill-Qwen-32B 在AIME 2024 上得分为 72.6% ，在MATH-500 上得分为 94.3% ，在LiveC\nodeBench 上得分为 57.2% 。这些结果显著优于之前的开源模型，并与 o1-mini 相当。我们向\n社区开源了基于 Qwen2.5 和Llama3 系列的1.5B 、7B 、8B 、14B 、32B 和70B 蒸馏检查点。\n1.2. 评估结果总结\n• 推理任务：（ 1 ）DeepSeek-R1 在 AIME 2024 上取得了  79.8% 的 Pass@1 分数，略微超过\n了 OpenAI-o1-1217 。在 MATH-500 上，它获得了令人印象深刻的  97.3% 的分数，与  Open\nAI-o1-1217 表现相当，并显著优于其他模型。（ 2 ）在与编码相关的任务中， DeepSeek-R1 \n在代码竞赛任务中展示了专家水平，它在  Codeforces 上获得了  2,029 的 Elo 评分，超过了  \n96.3% 的人类参赛者。对于工程相关任务， DeepSeek-R1 的表现略优于  DeepSeek-V3 ，这\n可能有助于开发人员在现实世界任务中取得更好的成果。\n• 知识：在 MMLU 、MMLU-Pro 和GPQA Diamond 等基准测试中， DeepSeek-R1 取得了出色的\n成绩，显著超越了 DeepSeek-V3 ，得分分别为 MMLU 90.8% 、MMLU-Pro 84.0% 和GPQA Di\namond 71.5% 。虽然在这些基准测试中其表现略低于 OpenAI-o1-1217 ，但DeepSeek-R1 超越\n了其他闭源模型，展示了其在教育任务中的竞争优势。在事实基准测试 SimpleQA 上，Dee\npSeek-R1 的表现优于 DeepSeek-V3 ，展示了其处理基于事实的查询的能力。类似趋势在 Ope\nnAI-o1 超越4o 的基准测试中也有所体现。\n4",
      "metadata": {
        "chunk_id": 4,
        "page_number": 4,
        "page_range": "4",
        "word_count": 157
      }
    },
    {
      "content": "• 其他方面： DeepSeek-R1 在多种任务中也表现出色，包括创意写作、通用问答、编辑、摘\n要等。它在  AlpacaEval 2.0 上实现了  87.6% 的长度控制胜率，在  Are-naHard 上实现了  92.3\n% 的胜率，展示了其智能处理非应试查询的强大能力。此外， DeepSeek-R1 在需要长上下\n文理解的任务上表现出色，在长上下文基准测试中显著优于  DeepSeek-V3 。\n2. 方法\n2.1. 概述\n先前的工作严重依赖大量监督数据来提升模型性能。在本研究中，我们证明了即使不使用监督\n微调（SFT ）作为冷启动，通过大规模强化学习（ RL ）也能显著提升推理能力。此外，加入少\n量冷启动数据可以进一步提升性能。在接下来的章节中，我们将介绍：（ 1 ）DeepSeek-R1-Zero\n，它直接将 RL 应用于基础模型，不使用任何 SFT 数据；（ 2 ）DeepSeek-R1 ，它从经过数千个长\n链思维（ CoT ）示例微调的检查点开始应用 RL ；（3 ）将DeepSeek-R1 的推理能力蒸馏到小型密\n集模型中。\n2.2. DeepSeek-R1-Zero ：基础模型上的强化学习\n强化学习在推理任务中展现了显著的有效性，正如我们之前的工作所证明的那样（ Shao 等，202\n4 ；Wang 等，2023 ）。然而，这些工作严重依赖于监督数据，而这些数据的收集非常耗时。在\n本节中，我们探索了大型语言模型（ LLMs ）在没有监督数据的情况下发展推理能力的潜力，重\n点关注它们通过纯强化学习过程的自我进化。我们首先简要概述了我们的强化学习算法，随后\n展示了一些令人兴奋的结果，并希望这能为社区提供有价值的见解。\n2.2.1.ReinforcementLearningAlgorithm\n群体相对策略优化  为了节省强化学习的训练成本，我们采用了群体相对策略优化（ GRPO ）（S\nhao 等，2024 ），该方法放弃了通常与策略模型大小相同的评论家模型，转而从群体分数中估计\n基线。具体来说，对于每个问题 𝑞 ，GRPO 从旧策略𝜋𝜃𝑜𝑙𝑑 中采样一组输出{𝑜1 、𝑜2 、··· 、𝑜𝐺} ，然后\n通过最大化以下目标来优化策略模型 𝜋𝜃 ：\nJ𝐺𝑅𝑃𝑂(𝜃)=E[𝑞∼𝑃(𝑄),{𝑜𝑖}𝐺\n𝑖=1∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)]\n1\n𝐺𝐺∑︁\n𝑖=1\u0012\nmin\u0012𝜋𝜃(𝑜𝑖|𝑞)\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞)𝐴𝑖,clip\u0012𝜋𝜃(𝑜𝑖|𝑞)\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞),1−𝜀,1+𝜀\u0013\n𝐴𝑖\u0013\n−𝛽D𝐾𝐿\u0000\n𝜋𝜃||𝜋𝑟𝑒𝑓\u0001\u0013\n,(1)\nD𝐾𝐿\u0000\n𝜋𝜃||𝜋𝑟𝑒𝑓\u0001=𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\n𝜋𝜃(𝑜𝑖|𝑞)−log𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\n𝜋𝜃(𝑜𝑖|𝑞)−1, (2)\n其中 𝜀 和 𝛽 是超参数， 𝐴𝑖 是优势，通过使用一组奖励  {𝑟1 、𝑟2 、... 、𝑟𝐺} 来计算，这些奖励对应\n于每个组内的输出：\n𝐴𝑖=𝑟𝑖−m𝑒𝑎𝑛({𝑟1,𝑟2,···,𝑟𝐺})\ns𝑡𝑑({𝑟1,𝑟2,···,𝑟𝐺}). (3)\n5",
      "metadata": {
        "chunk_id": 5,
        "page_number": 5,
        "page_range": "5",
        "word_count": 117
      }
    }
  ]
}