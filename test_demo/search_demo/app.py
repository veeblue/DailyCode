import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, CSVLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain_core.prompts import PromptTemplate
import tempfile
import os
from langchain_tavily import TavilySearch
from loaders.csv_loader import MyCSVLoader
from loaders.pdf_loader import MyPdfLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åˆå§‹åŒ–è®¾ç½®
st.set_page_config(page_title="æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹", layout="wide")
st.title("ğŸ” æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ - PDF/CSVåˆ†æ + å®æ—¶æœç´¢")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "last_references" not in st.session_state:
    st.session_state.last_references = []

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("ğŸ“ æ–‡æ¡£ä¸Šä¼ ")
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ PDFæˆ–CSVæ–‡ä»¶",
        type=["pdf", "csv"],
        accept_multiple_files=True
    )

    # å¤„ç†ä¸Šä¼ æ–‡ä»¶
    if uploaded_files:
        documents = []
        for file in uploaded_files:
            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                tmp_file.write(file.getvalue())
                tmp_path = tmp_file.name

                try:
                    if file.name.endswith('.pdf'):
                        loader = MyPdfLoader(tmp_path)
                        documents.extend(loader.load())
                    elif file.name.endswith('.csv'):
                        loader = MyCSVLoader(tmp_path)
                        documents.extend(loader.load())
                except Exception as e:
                    st.error(f"å¤„ç† {file.name} æ—¶å‡ºé”™: {str(e)}")
                finally:
                    os.unlink(tmp_path)

        if documents:
            # æ–‡æ¡£åˆ‡å‰²
            splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            chunked_documents = splitter.split_documents(documents)
            # åˆ›å»ºå‘é‡å­˜å‚¨
            embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            st.session_state.vector_store = FAISS.from_documents(chunked_documents, embeddings)
            st.success(f"æˆåŠŸåŠ è½½ {len(chunked_documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ!")

    st.divider()
    st.markdown("### é…ç½®é€‰é¡¹")
    search_enabled = st.checkbox("å¯ç”¨Tavilyæœç´¢", value=True)
    chunk_size = st.number_input("åˆ†å—å¤§å° (chunk_size)", min_value=100, max_value=2000, value=500, step=100)
    chunk_overlap = st.number_input("åˆ†å—é‡å  (chunk_overlap)", min_value=0, max_value=500, value=50, step=10)
    st.divider()
    st.caption("æç¤ºï¼šä¸Šä¼ æ–‡æ¡£åå¯ç›´æ¥æé—®ç›¸å…³å†…å®¹ï¼Œæˆ–å¯ç”¨æœç´¢è·å–å®æ—¶ä¿¡æ¯")

# èŠå¤©å†å²æ˜¾ç¤º
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ç”¨æˆ·è¾“å…¥å¤„ç†
if prompt := st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # åˆå§‹åŒ–å·¥å…·åˆ—è¡¨
    tools = []

    # 1. æ–‡æ¡£æ£€ç´¢å·¥å…·
    if st.session_state.vector_store:
        doc_retriever = st.session_state.vector_store.as_retriever()
        def document_search_func(q):
            docs = doc_retriever.get_relevant_documents(q)
            st.session_state.last_references = [d.metadata for d in docs]
            return "\n\n".join([d.page_content for d in docs])
        tools.append(Tool(
            name="document_search",
            func=document_search_func,
            description="ä½¿ç”¨ä¸Šä¼ çš„PDF/CSVæ–‡æ¡£å›ç­”é—®é¢˜"
        ))

    # 2. Tavilyæœç´¢å·¥å…·
    if search_enabled:
        tavily_api_key = os.getenv("TAVILY_API_KEY")
        if tavily_api_key:
            tavily_tool = TavilySearch(api_key=tavily_api_key, max_results=5)
            tools.append(Tool(
                name="web_search",
                func=tavily_tool.run,
                description="ä½¿ç”¨Tavilyæœç´¢å¼•æ“è·å–å®æ—¶ä¿¡æ¯"
            ))
        else:
            st.warning("æœªè®¾ç½®Tavily APIå¯†é’¥")

    # æ— å¯ç”¨å·¥å…·æ—¶çš„å¤„ç†
    if not tools:
        response = "è¯·è‡³å°‘ä¸Šä¼ æ–‡æ¡£æˆ–å¯ç”¨æœç´¢åŠŸèƒ½"
    else:
        try:
            # åˆ›å»ºAgent
            llm = ChatOpenAI(
                model="deepseek-chat",
                temperature=0.7,
                openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
                openai_api_base="https://api.deepseek.com/v1",
                streaming=True,
            )

            # Agentæç¤ºæ¨¡æ¿
            # Agentæç¤ºæ¨¡æ¿éƒ¨åˆ†éœ€è¦ä¿®æ”¹ä¸ºï¼š
            template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ã€‚ä½¿ç”¨æä¾›çš„å·¥å…·å›ç­”é—®é¢˜ã€‚

            ä½ æ‹¥æœ‰ä»¥ä¸‹å·¥å…·ï¼š
            {tools}{tool_names}

            ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

            Question: éœ€è¦å›ç­”çš„è¾“å…¥é—®é¢˜
            Thought: æ€è€ƒä¸‹ä¸€æ­¥è¯¥åšä»€ä¹ˆ
            Action: è¦æ‰§è¡Œçš„åŠ¨ä½œï¼Œä¼˜å…ˆä½¿ç”¨document_searchï¼Œå¦‚æœå†…å®¹ä¸document_searchæ— å…³ï¼Œåˆ™ä½¿ç”¨web_search
            Action Input: åŠ¨ä½œçš„è¾“å…¥å†…å®¹
            Observation: åŠ¨ä½œçš„ç»“æœ
            ... (è¿™ä¸ªæ€è€ƒ/åŠ¨ä½œ/è¾“å…¥/è§‚å¯Ÿçš„è¿‡ç¨‹å¯ä»¥é‡å¤å¤šæ¬¡)
            Thought: æˆ‘ç°åœ¨çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†
            Final Answer: ç”¨ç®€æ´çš„è¯­è¨€å›ç­”åŸå§‹é—®é¢˜

            è®°ä½ï¼š
            1. å›ç­”å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼
            2. æ‰€æœ‰ä¸­æ–‡å†…å®¹éƒ½è¦ç”¨ä¸­æ–‡å›ç­”
            3. Final Answerå¿…é¡»ç®€æ´æ˜äº†

            Question: {input}
            {agent_scratchpad}"""

            prompt_template = PromptTemplate.from_template(template)  # ä½¿ç”¨from_templateæ–¹æ³•

            # ç„¶ååˆ›å»ºagentæ—¶çš„ä»£ç ä¿æŒä¸å˜
            agent = create_react_agent(llm, tools, prompt_template)
            # ä¿®æ”¹agent_executorçš„åˆ›å»ºéƒ¨åˆ†
            agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                verbose=True,
                handle_parsing_errors=True  # æ·»åŠ æ­¤å‚æ•°
            )

            response = agent_executor.invoke({"input": prompt})["output"]
        except Exception as e:
            response = f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"

    # æ˜¾ç¤ºAIå“åº”
    with st.chat_message("assistant"):
        st.markdown(response)
        # è¿½åŠ å‚è€ƒæ–‡æ¡£æ¥æº
        if st.session_state.last_references:
            st.markdown("\n---\n**å‚è€ƒæ–‡æ¡£æ¥æºï¼š**")
            for ref in st.session_state.last_references:
                file_name = ref.get('file_name') or ref.get('source') or 'æœªçŸ¥æ¥æº'
                page = ref.get('page_number')
                if page:
                    st.markdown(f"- {file_name} (ç¬¬{page}é¡µ)")
                else:
                    st.markdown(f"- {file_name}")
    st.session_state.messages.append({"role": "assistant", "content": response})